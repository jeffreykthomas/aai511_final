{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2640,
     "status": "ok",
     "timestamp": 1690349236460,
     "user": {
      "displayName": "Jeffrey Thomas",
      "userId": "00428686483222286344"
     },
     "user_tz": 420
    },
    "id": "tLcrNQckYNPd",
    "outputId": "ec0b2d69-c4ca-42af-9d50-21877a5e0422"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import pretty_midi\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from scipy.io import wavfile\n",
    "from python_speech_features import mfcc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, TensorDataset\n",
    "#from torcheval.metrics import \n",
    "\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Preproccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1690349299069,
     "user": {
      "displayName": "Jeffrey Thomas",
      "userId": "00428686483222286344"
     },
     "user_tz": 420
    },
    "id": "o5IyPvz4alv4"
   },
   "outputs": [],
   "source": [
    "raw_data_folder = 'Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LpQZNwgAo2LC"
   },
   "outputs": [],
   "source": [
    "def preprocess_midi_into_mel_and_mfcc(midi_file, segment_length=224, mfcc_segment_length=519, num_cepstral=13):\n",
    "    midi_data = pretty_midi.PrettyMIDI(midi_file)\n",
    "    audio_data = midi_data.synthesize()\n",
    "    wavfile.write(\"temp.wav\", 44100, audio_data.astype(np.float32))\n",
    "\n",
    "    audio, rate = librosa.load(\"temp.wav\")\n",
    "    spectrogram = librosa.feature.melspectrogram(y=audio, sr=rate, n_mels=224)\n",
    "    spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "\n",
    "    mfcc_features = mfcc(audio, samplerate=rate, numcep=num_cepstral, winlen=0.025, winstep=0.01, nfft=1103)\n",
    "\n",
    "    # Truncate the spectrogram and MFCC features\n",
    "    if spectrogram_db.shape[1] % segment_length != 0:\n",
    "            spectrogram_db = spectrogram_db[:, :-(spectrogram_db.shape[1] % segment_length)]\n",
    "\n",
    "    segments = []\n",
    "    for start in range(0, spectrogram_db.shape[1] - segment_length + 1, segment_length):\n",
    "        mel_segment = spectrogram_db[:, start:start + segment_length]\n",
    "        mfcc_start = mfcc_segment_length * (start // segment_length)\n",
    "        mfcc_end = mfcc_start + mfcc_segment_length\n",
    "        mfcc_segment = mfcc_features[mfcc_start:mfcc_end, :]\n",
    "        # Only consider segments that are long enough\n",
    "        if mfcc_segment.shape[0] == mfcc_segment_length:\n",
    "            segments.append((mel_segment, mfcc_segment))\n",
    "\n",
    "    os.remove(\"temp.wav\")\n",
    "\n",
    "    return segments, audio, rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "B8bu9-GBcQDB"
   },
   "outputs": [],
   "source": [
    "def preprocess_data_in_directory(base_dir):\n",
    "    y = []\n",
    "    X_mel = []\n",
    "    X_mfcc = []\n",
    "    composer_names = []\n",
    "    audio_time_series_list = []\n",
    "    sampling_rate_list = []\n",
    "\n",
    "    # to show progression\n",
    "    total_files = sum([len(files) for r, d, files in os.walk(base_dir)])\n",
    "    processed_files = 0\n",
    "\n",
    "    composers = os.listdir(base_dir)\n",
    "    for composer in composers:\n",
    "        composer_dir = os.path.join(base_dir, composer)\n",
    "        if os.path.isdir(composer_dir):\n",
    "            for file in os.listdir(composer_dir):\n",
    "                if file.endswith('.mid'):\n",
    "                    composer_names.append(composer)\n",
    "                    file_path = os.path.join(composer_dir, file)\n",
    "                    try:\n",
    "                        segments, audio_time_series, rate = preprocess_midi_into_mel_and_mfcc(file_path)\n",
    "                        audio_time_series_list.append(audio_time_series)\n",
    "                        sampling_rate_list.append(rate)\n",
    "\n",
    "                        for mel_segment, mfcc_segment in segments:\n",
    "                            X_mel.append(mel_segment)\n",
    "                            X_mfcc.append(mfcc_segment)\n",
    "                            y.append(composer)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {file_path}: {e}\")\n",
    "                    processed_files += 1\n",
    "                    if processed_files % (total_files // 10) == 0:\n",
    "                        print(f\"Processed {processed_files / total_files * 100}% of files\")\n",
    "    return np.array(X_mel), np.array(X_mfcc), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1202296,
     "status": "ok",
     "timestamp": 1689968687934,
     "user": {
      "displayName": "Jeffrey Thomas",
      "userId": "00428686483222286344"
     },
     "user_tz": 420
    },
    "id": "nKU-Hg2bccyZ",
    "outputId": "767d974d-74c7-420e-cb21-596b2ec81524"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 9.788359788359788% of files\n",
      "Processed 19.576719576719576% of files\n",
      "Processed 29.365079365079367% of files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\lib\\site-packages\\pretty_midi\\pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 39.15343915343915% of files\n",
      "Processed 48.94179894179894% of files\n",
      "Processed 58.730158730158735% of files\n",
      "Processed 68.51851851851852% of files\n",
      "Processed 78.3068783068783% of files\n",
      "Processed 88.09523809523809% of files\n",
      "Processed 9.090909090909092% of files\n",
      "Processed 18.181818181818183% of files\n",
      "Processed 27.27272727272727% of files\n",
      "Processed 36.36363636363637% of files\n",
      "Processed 45.45454545454545% of files\n",
      "Processed 54.54545454545454% of files\n",
      "Processed 63.63636363636363% of files\n",
      "Processed 72.72727272727273% of files\n",
      "Processed 9.090909090909092% of files\n",
      "Processed 18.181818181818183% of files\n",
      "Processed 27.27272727272727% of files\n",
      "Processed 36.36363636363637% of files\n",
      "Processed 45.45454545454545% of files\n",
      "Processed 54.54545454545454% of files\n",
      "Processed 63.63636363636363% of files\n",
      "Processed 72.72727272727273% of files\n"
     ]
    }
   ],
   "source": [
    "X_train_mel, X_train_mfcc, y_train, audio_time_series_train, sampling_rate_train = preprocess_data_in_directory(raw_data_folder + 'train')\n",
    "X_test_mel, X_test_mfcc, y_test, audio_time_series_test, sampling_rate_test = preprocess_data_in_directory(raw_data_folder + 'test')\n",
    "X_dev_mel, X_dev_mfcc, y_dev, audio_time_series_dev, sampling_rate_dev = preprocess_data_in_directory(raw_data_folder + 'dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Composer Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "y_dev = le.transform(y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Standardizing The Mel Spectrogram Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value_mel = X_train_mel.min()\n",
    "max_value_mel = X_train_mel.max()\n",
    "\n",
    "X_train_mel = (X_train_mel - min_value_mel) / (max_value_mel - min_value_mel)\n",
    "X_dev_mel = (X_dev_mel - min_value_mel) / (max_value_mel - min_value_mel)\n",
    "X_test_mel = (X_test_mel - min_value_mel) / (max_value_mel - min_value_mel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Results To Tensors For PyTorch and then save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy to Tensor\n",
    "X_train_mel = torch.from_numpy(X_train_mel).float()\n",
    "X_train_mfcc = torch.from_numpy(X_train_mfcc).float()\n",
    "\n",
    "X_train_mel = X_train_mel.unsqueeze(1)\n",
    "\n",
    "y_train = torch.from_numpy(y_train).long() # Save y labels as longs\n",
    "\n",
    "X_test_mel = torch.from_numpy(X_test_mel).float()\n",
    "X_test_mfcc = torch.from_numpy(X_test_mfcc).float()\n",
    "\n",
    "X_test_mel = X_test_mel.unsqueeze(1)\n",
    "\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "\n",
    "X_dev_mel = torch.from_numpy(X_dev_mel).float()\n",
    "X_dev_mfcc = torch.from_numpy(X_dev_mfcc).float()\n",
    "\n",
    "X_dev_mel = X_dev_mel.unsqueeze(1)\n",
    "\n",
    "y_dev = torch.from_numpy(y_dev).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add channel dimensions for use with pretrained models\n",
    "X_train_rgb = X_train_mel.repeat(1, 3, 1, 1)\n",
    "X_test_rgb = X_test_mel.repeat(1, 3, 1, 1)\n",
    "X_dev_rgb = X_dev_mel.repeat(1, 3, 1, 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2908,
     "status": "ok",
     "timestamp": 1690236595709,
     "user": {
      "displayName": "Jeffrey Thomas",
      "userId": "00428686483222286344"
     },
     "user_tz": 420
    },
    "id": "XMmpWmiDEfM-"
   },
   "outputs": [],
   "source": [
    "# Saving Results\n",
    "torch.save(X_train_rgb, raw_data_folder + 'X_train_rgb.pt')\n",
    "torch.save(X_train_mfcc, raw_data_folder + 'X_train_mfcc.pt')\n",
    "torch.save(y_train, raw_data_folder + 'y_train.pt')\n",
    "torch.save(X_test_rgb, raw_data_folder + 'X_test_rgb.pt')\n",
    "torch.save(X_test_mfcc, raw_data_folder + 'X_test_mfcc.pt')\n",
    "torch.save(y_test, raw_data_folder + 'y_test.pt')\n",
    "torch.save(X_dev_rgb, raw_data_folder + 'X_dev_rgb.pt')\n",
    "torch.save(X_dev_mfcc, raw_data_folder + 'X_dev_mfcc.pt')\n",
    "torch.save(y_dev, raw_data_folder + 'y_dev.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the preproccessing section the MFCC and Mel data was extracted from audio files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder = 'Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 2122,
     "status": "ok",
     "timestamp": 1690349322619,
     "user": {
      "displayName": "Jeffrey Thomas",
      "userId": "00428686483222286344"
     },
     "user_tz": 420
    },
    "id": "rsb3sZoIFSCw"
   },
   "outputs": [],
   "source": [
    "X_train_mel = torch.load(raw_data_folder + 'X_train_rgb.pt')\n",
    "X_train_mfcc = torch.load(raw_data_folder + 'X_train_mfcc.pt')\n",
    "y_train = torch.load(raw_data_folder + 'y_train.pt')\n",
    "\n",
    "X_test_mel = torch.load(raw_data_folder + 'X_test_rgb.pt')\n",
    "X_test_mfcc = torch.load(raw_data_folder + 'X_test_mfcc.pt')\n",
    "y_test = torch.load(raw_data_folder + 'y_test.pt')\n",
    "\n",
    "X_dev_mel = torch.load(raw_data_folder + 'X_dev_rgb.pt')\n",
    "X_dev_mfcc = torch.load(raw_data_folder + 'X_dev_mfcc.pt')\n",
    "y_dev = torch.load(raw_data_folder + 'y_dev.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Getting the Class List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = []\n",
    "composers = os.listdir(raw_data_folder + 'train')\n",
    "for composer in composers:\n",
    "    class_list.append(composer)\n",
    "class_list = class_list[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bach',\n",
       " 'bartok',\n",
       " 'byrd',\n",
       " 'chopin',\n",
       " 'handel',\n",
       " 'hummel',\n",
       " 'mendelssohn',\n",
       " 'mozart']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# accumulate composer names for visualizations\n",
    "composer_names = []\n",
    "composers = os.listdir(raw_data_folder + 'train')\n",
    "for composer in composers:\n",
    "    composer_dir = os.path.join(raw_data_folder + 'train', composer)\n",
    "    if os.path.isdir(composer_dir):\n",
    "        for file in os.listdir(composer_dir):\n",
    "            if file.endswith('.mid'):\n",
    "                file_path = os.path.join(composer_dir, file)\n",
    "                composer_names.append(composer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Mel Spectrogram's\n",
    "fig, axs = plt.subplots(2, figsize=(10, 8))\n",
    "\n",
    "# denormalize the first example\n",
    "X_train_denorm1 = X_train_rgb[1220][0].cpu() * (max_value_mel - min_value_mel) + min_value_mel\n",
    "\n",
    "# plot the first example\n",
    "im1 = axs[0].imshow(X_train_denorm1, cmap='jet', aspect='auto', origin='lower')\n",
    "axs[0].set_title(f'Mel spectrogram, {class_list[y_train[1220]]}')\n",
    "fig.colorbar(im1, ax=axs[0], format='%+2.0f dB')\n",
    "\n",
    "# denormalize the second example\n",
    "X_train_denorm2 = X_train_rgb[13450][0].cpu() * (max_value_mel - min_value_mel) + min_value_mel\n",
    "\n",
    "# plot the second example\n",
    "im2 = axs[1].imshow(X_train_denorm2, cmap='jet', aspect='auto', origin='lower')\n",
    "axs[1].set_title(f'Mel spectrogram, {class_list[y_train[13450]]}')\n",
    "fig.colorbar(im2, ax=axs[1], format='%+2.0f dB')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Example MFCC's\n",
    "fig, axs = plt.subplots(2, figsize=(10, 8))\n",
    "im1 = axs[0].imshow(X_train_mfcc[1220][0].cpu(), cmap='jet', aspect='auto', origin='lower')\n",
    "axs[0].set_title(f'MFCC, {class_list[y_train[1220]]}')\n",
    "\n",
    "im2 = axs[1].imshow(X_train_mfcc[13450][0].cpu(), cmap='jet', aspect='auto', origin='lower')\n",
    "axs[1].set_title(f'MFCC, {class_list[y_train[13450]]}')\n",
    "fig.colorbar(im2, ax=axs[1], format='%+2.0f dB')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_energy(mel_spectrogram):\n",
    "    return torch.sum(mel_spectrogram**2, axis=1)\n",
    "\n",
    "# Convert y_train to a numpy array\n",
    "y_train_np = np.array(y_train.cpu())\n",
    "\n",
    "# Compute the mean energy for each Mel-spectrogram in training set\n",
    "mean_energies = [torch.mean(compute_energy(mel_spectrogram)).item() for mel_spectrogram in X_train_mel]\n",
    "\n",
    "composer_names = [class_list[i] for i in y_train_np]\n",
    "data = pd.DataFrame({'Composer': composer_names, 'Mean Energy': mean_energies})\n",
    "\n",
    "# Group by composer and compute the mean of the mean energies\n",
    "grouped = data.groupby('Composer')['Mean Energy'].mean()\n",
    "\n",
    "# Plot a bar chart of the mean energies by composer\n",
    "grouped.plot(kind='bar')\n",
    "plt.ylabel('Mean Energy')\n",
    "plt.title('Mean Energy by Composer')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_spectral_contrast(mel_spectrogram):\n",
    "    # Convert the tensor to a numpy array and compute the spectral contrast\n",
    "    mel_spectrogram_np = mel_spectrogram.cpu().numpy()\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(S=mel_spectrogram_np)\n",
    "    return np.mean(spectral_contrast)\n",
    "\n",
    "# Compute the mean spectral contrast for each Mel-spectrogram in training set\n",
    "mean_spectral_contrasts = [compute_spectral_contrast(mel_spectrogram) for mel_spectrogram in X_train_mel]\n",
    "\n",
    "# Map integer labels to composer names\n",
    "composer_names = [class_list[i] for i in y_train_np]\n",
    "\n",
    "# Combine the composers and mean spectral contrasts into a DataFrame\n",
    "data = pd.DataFrame({'Composer': composer_names, 'Mean Spectral Contrast': mean_spectral_contrasts})\n",
    "\n",
    "# Group by composer and compute the mean of the mean spectral contrasts\n",
    "grouped = data.groupby('Composer')['Mean Spectral Contrast'].mean()\n",
    "\n",
    "# Plot a bar chart of the mean spectral contrasts by composer\n",
    "grouped.plot(kind='bar')\n",
    "plt.ylabel('Mean Spectral Contrast')\n",
    "plt.title('Mean Spectral Contrast by Composer')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate chroma features for each audio file\n",
    "chroma_features = [librosa.feature.chroma_stft(y=audio_time_series, sr=sampling_rate) for audio_time_series, sampling_rate in zip(audio_time_series_train, sampling_rate_train)]\n",
    "\n",
    "# Compute the mean chroma feature for each audio file\n",
    "mean_chroma_features = [np.mean(chroma_feature, axis=1) for chroma_feature in chroma_features]\n",
    "\n",
    "# Convert the list of mean chroma features to a 2D array\n",
    "mean_chroma_features_array = np.array(mean_chroma_features)\n",
    "\n",
    "# Combine the composers and mean chroma features into a DataFrame\n",
    "data = pd.DataFrame(mean_chroma_features_array, columns=[f'Chroma {i+1}' for i in range(12)])\n",
    "data['Composer'] = composer_names\n",
    "\n",
    "# Melt the DataFrame to long format for plotting\n",
    "data_melted = pd.melt(data, id_vars='Composer', var_name='Chroma', value_name='Mean Chroma Feature')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot a boxplot of the mean chroma features by composer\n",
    "plt.figure(figsize=(15,8))\n",
    "ax = sns.boxplot(x='Composer', y='Mean Chroma Feature', hue='Chroma', data=data_melted)\n",
    "ax.legend_.remove()\n",
    "plt.title('Mean Chroma Features by Composer')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Weighted Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1690349325584,
     "user": {
      "displayName": "Jeffrey Thomas",
      "userId": "00428686483222286344"
     },
     "user_tz": 420
    },
    "id": "1XpHl97poGMg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 234\n",
      "1: 111\n",
      "2: 194\n",
      "3: 221\n",
      "4: 197\n",
      "5: 500\n",
      "6: 227\n",
      "7: 425\n",
      "8: 265\n"
     ]
    }
   ],
   "source": [
    "# Get unique values and their counts\n",
    "unique_values, counts = torch.unique(y_train.cpu(), return_counts=True)\n",
    "\n",
    "# Print unique values and their counts\n",
    "for value, count in zip(unique_values, counts):\n",
    "    print(f'{value}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2374"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting Class Weights\n",
    "target = torch.Tensor.numpy(y_train.cpu())\n",
    "target = target.astype('int')\n",
    "class_sample_counts = np.array([len(np.where(target == t)[0]) for t in np.unique(target)])\n",
    "class_weights = [1/class_sample_counts[i] for i in target]\n",
    "len(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = WeightedRandomSampler(weights = class_weights, num_samples = len(class_weights), replacement = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Mel DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelDataset(Dataset):\n",
    "    def __init__(self, X_mel, y, class_list):\n",
    "        self.X_mel = X_mel\n",
    "        self.y = y\n",
    "        self.class_list = class_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_mel)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_mel[idx], self.y[idx]\n",
    "\n",
    "    def classes(self):\n",
    "        # Return the list of class labels\n",
    "        return self.class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_mel = MelDataset(X_train_mel, y_train, class_list)\n",
    "test_dataset_mel = MelDataset(X_test_mel, y_test, class_list)\n",
    "val_dataset_mel = MelDataset(X_dev_mel, y_dev, class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Weighted Sampling\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader\n",
    "train_loader_mel = DataLoader(train_dataset_mel, batch_size=batch_size, shuffle=True)\n",
    "test_loader_mel = DataLoader(test_dataset_mel, batch_size=batch_size, shuffle=False)\n",
    "val_loader_mel = DataLoader(val_dataset_mel, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Weighted Sampling\n",
    "weighted_train_loader_mel = DataLoader(train_dataset_mel, sampler = sampler,  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Making MFCC Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCCDataset(Dataset):\n",
    "    def __init__(self, X_mfcc, y, class_list):\n",
    "        self.X_mfcc = X_mfcc\n",
    "        self.y = y\n",
    "        self.class_list = class_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_mfcc)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_mfcc[idx], self.y[idx]\n",
    "\n",
    "    def classes(self):\n",
    "        # Return the list of class labels\n",
    "        return self.class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_mfcc = MFCCDataset(X_train_mfcc, y_train, class_list) \n",
    "test_dataset_mfcc = MFCCDataset(X_test_mfcc, y_test, class_list) \n",
    "val_dataset_mfcc = MFCCDataset(X_dev_mfcc, y_dev, class_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Weighted Sampling\n",
    "train_loader_mfcc = DataLoader(train_dataset_mfcc, batch_size=batch_size, shuffle=True)\n",
    "test_loader_mfcc = DataLoader(test_dataset_mfcc, batch_size=batch_size, shuffle=False)\n",
    "val_loader_mfcc = DataLoader(val_dataset_mfcc, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Weighted Sampling\n",
    "weighted_train_loader_mfcc = DataLoader(train_dataset_mfcc, sampler = sampler,  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making MEL and MFCC Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, X_mel, X_mfcc, y, class_list):\n",
    "        self.X_mel = X_mel\n",
    "        self.X_mfcc = X_mfcc\n",
    "        self.y = y\n",
    "        self.class_list = class_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_mfcc)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_mel[idx], self.X_mfcc[idx], self.y[idx]\n",
    "\n",
    "    def classes(self):\n",
    "        # Return the list of class labels\n",
    "        return self.class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_combined = CombinedDataset(X_train_mel, X_train_mfcc, y_train, class_list)\n",
    "test_dataset_combined = CombinedDataset(X_test_mel, X_test_mfcc, y_test, class_list)\n",
    "val_dataset_combined = CombinedDataset(X_dev_mel, X_dev_mfcc, y_dev, class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Weighted Sampling\n",
    "train_loader_combined = DataLoader(train_dataset_combined, batch_size=batch_size, shuffle=True)\n",
    "test_loader_combined = DataLoader(test_dataset_combined, batch_size=batch_size, shuffle=False)\n",
    "val_loader_combined = DataLoader(val_dataset_combined, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Weighted Sampling\n",
    "weighted_train_loader_combined = DataLoader(train_dataset_combined, sampler = sampler,  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MEL_CNN(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, init_filters, num_classes, dropout_rate=0.2):\n",
    "        super(MEL_CNN, self).__init__()\n",
    "        self.hidden_layers = num_hidden_layers\n",
    "        \n",
    "        #Pooling Layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Input Layer\n",
    "        self.layers = nn.ModuleDict()\n",
    "        self.layers[\"input_Conv\"] = nn.Conv2d(3, init_filters, kernel_size=3, padding=1)\n",
    "        self.layers[\"input_batch_norm\"] = nn.BatchNorm2d(init_filters)\n",
    "        \n",
    "        # Hidden Layers\n",
    "        for i in range(num_hidden_layers):\n",
    "            in_channels = 2 ** i * init_filters\n",
    "            out_channels = in_channels * 2\n",
    "            self.layers[f\"hidden_{i}\"] = nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1)\n",
    "            self.layers[f\"batch_norm_{i}\"] = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Last Layer\n",
    "        final_in_channels = 2 ** num_hidden_layers * init_filters\n",
    "        final_out_channels = final_in_channels * 4\n",
    "        self.layers[\"output_Conv\"] = nn.Conv2d(final_in_channels, final_out_channels, kernel_size = 1)\n",
    "        self.layers[\"output_batch_norm\"] = nn.BatchNorm2d(final_out_channels)\n",
    "        self.layers[\"linear\"] = nn.Linear(final_out_channels, 128)\n",
    "        self.layers[\"dropout\"] = nn.Dropout(p = dropout_rate)\n",
    "        self.layers[\"linear_output_1\"] = nn.Linear(128, 64)\n",
    "        self.layers[\"linear_output_2\"] = nn.Linear(64, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.layers[\"input_batch_norm\"](self.layers[\"input_Conv\"](x))))\n",
    "        for i in range(self.hidden_layers):\n",
    "            x = self.pool(nn.functional.relu(self.layers[f\"batch_norm_{i}\"](self.layers[f\"hidden_{i}\"](x))))\n",
    "            \n",
    "        x = self.pool(nn.functional.relu(self.layers[\"output_batch_norm\"](self.layers[\"output_Conv\"](x))))\n",
    "        x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = nn.functional.relu(self.layers[\"linear\"](x))\n",
    "        x = self.layers[\"dropout\"](x)\n",
    "        x = nn.functional.relu(self.layers[\"linear_output_1\"](x))\n",
    "        x = self.layers[\"linear_output_2\"](x)\n",
    "        return x # Don't need softmax in the forward function since we're using cross-entropy loss, which implicitly performs softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCC_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(MFCC_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(x.device)  # Multiply by 2 because it's bidirectional\n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(x.device)  # Multiply by 2 because it's bidirectional\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # shape = (batch_size, seq_length, hidden_size*2)\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM_Ensemble(nn.Module):\n",
    "    def __init__(self, model_CNN, model_LSTM, num_class):\n",
    "        super(CNN_LSTM_Ensemble, self).__init__()\n",
    "        self.LSTM = model_LSTM\n",
    "        self.CNN = model_CNN\n",
    "\n",
    "        fc_input_size = model_CNN.final_out_channel + model_LSTM.hidden_size*2\n",
    "        \n",
    "        # Define additional fully-connected layers\n",
    "        self.fc1 = nn.Linear(fc_input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "    \n",
    "    def forward(self, MEL, MFCC):\n",
    "        output_CNN = self.CNN(MEL)\n",
    "        output_LSTM = self.LSTM(MFCC)\n",
    "        output = torch.cat((output_CNN, output_LSTM), dim = 1)\n",
    "        \n",
    "        output = nn.functional.relu(self.fc1(output))\n",
    "        output = nn.functional.relu(self.fc2(output))\n",
    "        output = self.fc3(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, scheduler, train_loader, test_loader, ensemble=False, num_epochs=100, patience=10, model_id = \"Best_Model.pt\"):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    best_test_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            if ensemble:\n",
    "                mel_inputs, mfcc_inputs, labels = batch\n",
    "                mel_inputs, mfcc_inputs, labels = mel_inputs.to(device), mfcc_inputs.to(device), labels.to(device)\n",
    "                outputs = model(mel_inputs, mfcc_inputs)\n",
    "            else:\n",
    "                inputs, labels = batch\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            labels = labels.long()\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * mel_inputs.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        train_accuracy = train_correct / train_total\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0.0\n",
    "            test_correct = 0\n",
    "            test_total = 0\n",
    "\n",
    "            for batch in test_loader:\n",
    "                if ensemble:\n",
    "                    mel_inputs, mfcc_inputs, labels = batch\n",
    "                    mel_inputs, mfcc_inputs, labels = mel_inputs.to(device), mfcc_inputs.to(device), labels.to(device)\n",
    "                    outputs = model(mel_inputs, mfcc_inputs)\n",
    "                else:\n",
    "                    inputs, labels = batch\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Compute accuracy and loss\n",
    "                test_loss += criterion(outputs, labels).item() * mel_inputs.size(0)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            test_loss /= len(test_loader.dataset)\n",
    "            test_losses.append(test_loss)\n",
    "\n",
    "            test_accuracy = test_correct / test_total\n",
    "            test_accuracies.append(test_accuracy)\n",
    "\n",
    "        print(f'Epoch {epoch}, train loss: {train_loss:.4f}, train accuracy: {train_accuracy:.2f}, test loss: {test_loss:.4f}, test accuracy: {test_accuracy:.2f}, learning rate: {scheduler.get_lr()[0]}')\n",
    "\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            epochs_no_improve = 0\n",
    "            print('Saving model!')\n",
    "            torch.save(model.state_dict(), model_id)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve in [20, 30, 40]:\n",
    "                print('Stepping lr_scheduler')\n",
    "                scheduler.step()\n",
    "            if epochs_no_improve == patience:\n",
    "                print('Early stopping!')\n",
    "                break\n",
    "    results = {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'test_losses': test_losses,\n",
    "        'test_accuracies': test_accuracies,\n",
    "        'best_test_loss': best_test_loss\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Test CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (layers): ModuleDict(\n",
      "    (input_Conv): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (input_batch_norm): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (hidden_0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batch_norm_0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (hidden_1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (output_Conv): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (output_batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (linear_output_1): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (linear_output_2): Linear(in_features=64, out_features=9, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "hl = 2\n",
    "init = 8\n",
    "num_classes = len(torch.unique(y_train.cpu()))\n",
    "dropout = 0.5\n",
    "test_model = MEL_CNN(hl, init, num_classes, dropout)\n",
    "\n",
    "print(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 10\n",
    "num_epochs = 50\n",
    "lr = 0.001\n",
    "weight_decay=0.005\n",
    "optimizer = torch.optim.Adam(test_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "test_model, optimizer, weighted_train_loader_mel, test_loader_mel = accelerator.prepare(\n",
    "test_model, optimizer, weighted_train_loader_mel, test_loader_mel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 1.9713, train accuracy: 0.40, test loss: 2.0165, test accuracy: 0.38, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 1, train loss: 1.9622, train accuracy: 0.41, test loss: 2.0151, test accuracy: 0.34, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 2, train loss: 2.0005, train accuracy: 0.36, test loss: 2.0101, test accuracy: 0.35, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 3, train loss: 1.9742, train accuracy: 0.40, test loss: 1.9875, test accuracy: 0.37, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 4, train loss: 1.9774, train accuracy: 0.40, test loss: 1.9772, test accuracy: 0.40, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 5, train loss: 1.9762, train accuracy: 0.40, test loss: 1.9916, test accuracy: 0.38, learning rate: 0.001\n",
      "Epoch 6, train loss: 1.9750, train accuracy: 0.40, test loss: 1.9766, test accuracy: 0.40, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 7, train loss: 1.9761, train accuracy: 0.40, test loss: 1.9733, test accuracy: 0.41, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 8, train loss: 1.9836, train accuracy: 0.39, test loss: 1.9835, test accuracy: 0.40, learning rate: 0.001\n",
      "Epoch 9, train loss: 1.9752, train accuracy: 0.40, test loss: 2.1336, test accuracy: 0.18, learning rate: 0.001\n",
      "Epoch 10, train loss: 1.9697, train accuracy: 0.41, test loss: 1.9695, test accuracy: 0.44, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 11, train loss: 1.9670, train accuracy: 0.41, test loss: 2.0757, test accuracy: 0.32, learning rate: 0.001\n",
      "Epoch 12, train loss: 1.9605, train accuracy: 0.42, test loss: 2.0627, test accuracy: 0.27, learning rate: 0.001\n",
      "Epoch 13, train loss: 1.9639, train accuracy: 0.41, test loss: 1.9993, test accuracy: 0.35, learning rate: 0.001\n",
      "Epoch 14, train loss: 1.9659, train accuracy: 0.40, test loss: 1.9719, test accuracy: 0.39, learning rate: 0.001\n",
      "Epoch 15, train loss: 1.9512, train accuracy: 0.43, test loss: 1.9752, test accuracy: 0.39, learning rate: 0.001\n",
      "Epoch 16, train loss: 1.9625, train accuracy: 0.41, test loss: 2.0049, test accuracy: 0.36, learning rate: 0.001\n",
      "Epoch 17, train loss: 1.9614, train accuracy: 0.42, test loss: 2.0183, test accuracy: 0.35, learning rate: 0.001\n",
      "Epoch 18, train loss: 1.9633, train accuracy: 0.41, test loss: 2.0179, test accuracy: 0.36, learning rate: 0.001\n",
      "Epoch 19, train loss: 1.9582, train accuracy: 0.43, test loss: 2.0728, test accuracy: 0.27, learning rate: 0.001\n",
      "Epoch 20, train loss: 1.9670, train accuracy: 0.41, test loss: 1.9920, test accuracy: 0.40, learning rate: 0.001\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "cnn_results = train_model(test_model, optimizer, scheduler, weighted_train_loader_mel, test_loader_mel, ensemble=False, num_epochs=num_epochs, patience=patience, model_id=\"Best_MEL_CNN.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Test LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCC_LSTM(\n",
      "  (lstm): LSTM(13, 32, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=64, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 13\n",
    "hidden_size = 32\n",
    "num_layers = 2\n",
    "num_classes = len(torch.unique(y_train.cpu()))\n",
    "\n",
    "lstm_test_model = MFCC_LSTM(input_size, hidden_size, num_layers, num_classes)\n",
    "\n",
    "print(lstm_test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 10\n",
    "num_epochs = 50\n",
    "lr = 0.001\n",
    "weight_decay = 0.01\n",
    "\n",
    "optimizer = torch.optim.Adam(lstm_test_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "lstm_test_model, optimizer, weighted_train_loader_mfcc, test_loader_mfcc = accelerator.prepare(\n",
    "lstm_test_model, optimizer, weighted_train_loader_mfcc, test_loader_mfcc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 2.1945, train accuracy: 0.14, test loss: 2.1974, test accuracy: 0.11, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 1, train loss: 2.1787, train accuracy: 0.17, test loss: 2.1882, test accuracy: 0.16, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 2, train loss: 2.1482, train accuracy: 0.17, test loss: 2.1825, test accuracy: 0.13, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 3, train loss: 2.1066, train accuracy: 0.18, test loss: 2.1300, test accuracy: 0.15, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 4, train loss: 2.1332, train accuracy: 0.16, test loss: 2.1464, test accuracy: 0.10, learning rate: 0.001\n",
      "Epoch 5, train loss: 2.0835, train accuracy: 0.19, test loss: 2.1280, test accuracy: 0.10, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 6, train loss: 2.0502, train accuracy: 0.20, test loss: 2.1192, test accuracy: 0.13, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 7, train loss: 2.1069, train accuracy: 0.17, test loss: 2.1305, test accuracy: 0.14, learning rate: 0.001\n",
      "Epoch 8, train loss: 2.0446, train accuracy: 0.21, test loss: 2.1229, test accuracy: 0.12, learning rate: 0.001\n",
      "Epoch 9, train loss: 2.0389, train accuracy: 0.21, test loss: 2.1471, test accuracy: 0.08, learning rate: 0.001\n",
      "Epoch 10, train loss: 2.1089, train accuracy: 0.17, test loss: 2.1478, test accuracy: 0.15, learning rate: 0.001\n",
      "Epoch 11, train loss: 2.0407, train accuracy: 0.19, test loss: 1.9792, test accuracy: 0.18, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 12, train loss: 1.9878, train accuracy: 0.19, test loss: 1.9382, test accuracy: 0.17, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 13, train loss: 1.9396, train accuracy: 0.21, test loss: 2.1063, test accuracy: 0.16, learning rate: 0.001\n",
      "Epoch 14, train loss: 2.1065, train accuracy: 0.16, test loss: 2.1168, test accuracy: 0.11, learning rate: 0.001\n",
      "Epoch 15, train loss: 2.0401, train accuracy: 0.19, test loss: 2.0872, test accuracy: 0.11, learning rate: 0.001\n",
      "Epoch 16, train loss: 1.9818, train accuracy: 0.19, test loss: 2.0662, test accuracy: 0.11, learning rate: 0.001\n",
      "Epoch 17, train loss: 2.0847, train accuracy: 0.20, test loss: 2.0351, test accuracy: 0.13, learning rate: 0.001\n",
      "Epoch 18, train loss: 1.9933, train accuracy: 0.22, test loss: 1.9916, test accuracy: 0.15, learning rate: 0.001\n",
      "Epoch 19, train loss: 1.9690, train accuracy: 0.23, test loss: 1.9850, test accuracy: 0.16, learning rate: 0.001\n",
      "Epoch 20, train loss: 1.9838, train accuracy: 0.22, test loss: 2.0107, test accuracy: 0.16, learning rate: 0.001\n",
      "Epoch 21, train loss: 1.9691, train accuracy: 0.23, test loss: 1.9560, test accuracy: 0.13, learning rate: 0.001\n",
      "Epoch 22, train loss: 1.9555, train accuracy: 0.22, test loss: 1.9823, test accuracy: 0.13, learning rate: 0.001\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "lstm_results = train_model(lstm_test_model, optimizer, scheduler, weighted_train_loader_mfcc, test_loader_mfcc, ensemble=False, num_epochs=num_epochs, patience=patience, model_id=\"Best_MFCC_LSTM.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Testing Ensemble Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEL_CNN(\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (layers): ModuleDict(\n",
      "    (input_Conv): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (input_batch_norm): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (hidden_0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batch_norm_0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (hidden_1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (output_Conv): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (output_batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (linear_output_1): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (linear_output_2): Linear(in_features=64, out_features=9, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Mel Model\n",
    "hl = 2\n",
    "init = 8\n",
    "num_classes = len(torch.unique(y_train.cpu()))\n",
    "dropout = 0.5\n",
    "mel_model = MEL_CNN(hl, init, num_classes, dropout)\n",
    "\n",
    "print(mel_model)\n",
    "\n",
    "mel_model = mel_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCC_LSTM(\n",
      "  (lstm): LSTM(13, 32, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=64, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# MFCC model\n",
    "input_size = 13\n",
    "hidden_size = 32\n",
    "num_layers = 2\n",
    "num_classes = len(torch.unique(y_train.cpu()))\n",
    "\n",
    "mfcc_model = MFCC_LSTM(input_size, hidden_size, num_layers, num_classes)\n",
    "\n",
    "print(mfcc_model)\n",
    "mfcc_model = mfcc_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_LSTM_Ensemble(\n",
      "  (LSTM): MFCC_LSTM(\n",
      "    (lstm): LSTM(13, 32, num_layers=2, batch_first=True, bidirectional=True)\n",
      "    (fc): Linear(in_features=64, out_features=9, bias=True)\n",
      "  )\n",
      "  (CNN): MEL_CNN(\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (layers): ModuleDict(\n",
      "      (input_Conv): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (input_batch_norm): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (hidden_0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (batch_norm_0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (hidden_1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (output_Conv): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (output_batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "      (linear_output_1): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (linear_output_2): Linear(in_features=64, out_features=9, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (outward): Linear(in_features=9, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Ensemble\n",
    "test_ensemble = CNN_LSTM_Ensemble(mel_model, mfcc_model, num_classes)\n",
    "print(test_ensemble)\n",
    "test_ensemble = test_ensemble.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 10\n",
    "num_epochs = 50\n",
    "lr = 0.001\n",
    "weight_decay = 0.01\n",
    "\n",
    "optimizer = torch.optim.Adam(test_ensemble.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "test_ensemble, optimizer, weighted_train_loader_combined, test_loader_combined = accelerator.prepare(\n",
    "test_ensemble, optimizer, weighted_train_loader_combined, test_loader_combined\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 2.1979, train accuracy: 0.10, test loss: 2.1982, test accuracy: 0.05, learning rate: 0.001\n",
      "Saving model!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[122], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m train_losses, test_losses, train_acc, test_acc, best_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_ensemble\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_ensemble\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweighted_train_loader_combined\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader_combined\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpatience\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[104], line 81\u001B[0m, in \u001B[0;36mtrain_ensemble\u001B[1;34m(model, optimizer, scheduler, train_loader, test_loader, num_epochs, patience)\u001B[0m\n\u001B[0;32m     79\u001B[0m     epochs_no_improve \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     80\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSaving model!\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 81\u001B[0m     torch\u001B[38;5;241m.\u001B[39msave(model\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[43mmodel_id\u001B[49m)\n\u001B[0;32m     82\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     83\u001B[0m     epochs_no_improve \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model_id' is not defined"
     ]
    }
   ],
   "source": [
    "ensemble_results = train_model(test_ensemble, optimizer, scheduler, weighted_train_loader_combined, test_loader_combined, ensemble=True, num_epochs=num_epochs, patience=patience, model_id=\"Best_Ensemble.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, test_loader, ensemble=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            if ensemble:\n",
    "                mel_inputs, mfcc_inputs, labels = batch\n",
    "                mel_inputs = mel_inputs.to(device)  # Move inputs to the device\n",
    "                mfcc_inputs = mfcc_inputs.to(device)\n",
    "                labels = labels.to(device)  # Move labels to the device\n",
    "                outputs = model(mel_inputs, mfcc_inputs)\n",
    "            else:\n",
    "                inputs, labels = batch\n",
    "                inputs = inputs.to(device)  # Move inputs to the device\n",
    "                labels = labels.to(device)  # Move labels to the device\n",
    "                outputs = model(inputs)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Update counts\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cnn_val_accuracy = calculate_accuracy(cnn_model, val_loader_mel)\n",
    "print(f'CNN Validation Accuracy: {cnn_val_accuracy:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lstm_val_accuracy = calculate_accuracy(lstm_model, val_loader_mfcc)\n",
    "print(f'LSMT Validation Accuracy: {lstm_val_accuracy:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ensemble_val_accuracy = calculate_accuracy(ensemble_model, val_loader_combined, ensemble=True)\n",
    "print(f'Ensemble Validation Accuracy: {ensemble_val_accuracy:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(model, test_loader, num_classes):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for mel_inputs, labels in test_loader:\n",
    "            mel_inputs = mel_inputs.to(device)  # Move inputs to the device\n",
    "            labels = labels.to(device)  # Move labels to the device\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(mel_inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Create the confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions, labels=np.arange(num_classes))\n",
    "    return cm\n",
    "\n",
    "def plot_confusion_matrix(confusion_mtx, labels):\n",
    "    # Normalize the confusion matrix\n",
    "    confusion_mtx = confusion_mtx.astype('float') / confusion_mtx.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    # Create a heatmap using seaborn\n",
    "    sns.heatmap(confusion_mtx, annot=True, fmt='.2f', cmap='Blues', xticklabels=labels, yticklabels=labels, ax=ax)\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "\n",
    "    # Rotate x-axis labels if needed\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(torch.unique(y_dev))\n",
    "confusion_mtx = create_confusion_matrix(model, val_loader, num_classes)\n",
    "class_labels = val_loader.dataset.classes()\n",
    "plot_confusion_matrix(confusion_mtx, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader_mel:\n",
    "        inputs = inputs.to(device)  # Move inputs to the device (e.g., GPU)\n",
    "        labels = labels.to(device)  # Move labels to the device\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = mel_model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Update counts\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader_mel:\n",
    "        inputs = inputs.to(device)  # Move inputs to the device (e.g., GPU)\n",
    "        labels = labels.to(device)  # Move labels to the device\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = mel_model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_predictions, labels=np.arange(num_classes))\n",
    "\n",
    "# Normalize the confusion matrix\n",
    "confusion_mtx = confusion_mtx.astype('float') / confusion_mtx.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "\n",
    "class_labels = val_loader.dataset.classes()\n",
    "# Create a heatmap using seaborn\n",
    "sns.heatmap(confusion_mtx, annot=True, fmt='.2f', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels, ax=ax)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels')\n",
    "ax.set_title('Confusion Matrix')\n",
    "\n",
    "# Rotate x-axis labels if needed\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOUJWzLCGobAI546zASPdEC",
   "mount_file_id": "1BRNgN_JKDJfHFffqSAc_mckufzMmrrpO",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
