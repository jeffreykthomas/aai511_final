{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2640,
     "status": "ok",
     "timestamp": 1690349236460,
     "user": {
      "displayName": "Jeffrey Thomas",
      "userId": "00428686483222286344"
     },
     "user_tz": 420
    },
    "id": "tLcrNQckYNPd",
    "outputId": "ec0b2d69-c4ca-42af-9d50-21877a5e0422"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import pretty_midi\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from scipy.io import wavfile\n",
    "from python_speech_features import mfcc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, TensorDataset\n",
    "from torcheval.metrics import MulticlassAccuracy, MulticlassF1Score, MulticlassAUROC\n",
    "\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Preproccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1690349299069,
     "user": {
      "displayName": "Jeffrey Thomas",
      "userId": "00428686483222286344"
     },
     "user_tz": 420
    },
    "id": "o5IyPvz4alv4"
   },
   "outputs": [],
   "source": [
    "# Change this to directory where the data is stored\n",
    "raw_data_folder = 'Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose(midi_file, semitones=None):\n",
    "    \"\"\"\n",
    "    Transpose a MIDI file by a given number of semitones.\n",
    "    \"\"\"\n",
    "    if semitones is None:\n",
    "        semitones = np.random.choice(list(range(-3, 0)) + list(range(1, 4)))\n",
    "    midi_data = pretty_midi.PrettyMIDI(midi_file)\n",
    "    for instrument in midi_data.instruments:\n",
    "        for note in instrument.notes:\n",
    "            note.pitch += semitones\n",
    "    return midi_data\n",
    "\n",
    "def change_tempo(midi_file, factor=None):\n",
    "    if factor is None:\n",
    "        factor = np.random.uniform(0.5, 1.5)\n",
    "    midi_data = pretty_midi.PrettyMIDI(midi_file)\n",
    "    for tempo_change in midi_data.get_tempo_changes()[1]:\n",
    "        tempo_change /= factor\n",
    "    return midi_data\n",
    "\n",
    "def rhythmic_variation(midi_file, max_variation=None):\n",
    "    \"\"\"\n",
    "    Add a random variation to the start and end times of each note.\n",
    "    max_variation is in milliseconds.\n",
    "    \"\"\"\n",
    "    if max_variation is None:\n",
    "        max_variation = np.random.randint(5, 15)\n",
    "    midi_data = pretty_midi.PrettyMIDI(midi_file)\n",
    "    for instrument in midi_data.instruments:\n",
    "        for note in instrument.notes:\n",
    "            variation = np.random.uniform(-max_variation, max_variation) / 1000.0  # convert to seconds\n",
    "            note.start += variation\n",
    "            note.end += variation\n",
    "            # ensure start is not negative\n",
    "            note.start = max(0, note.start)\n",
    "            # ensure end is not less than start\n",
    "            note.end = max(note.start, note.end)\n",
    "    return midi_data\n",
    "\n",
    "def drop_notes(midi_file, drop_probability=None):\n",
    "    \"\"\"\n",
    "    Randomly drop notes with a certain probability.\n",
    "    \"\"\"\n",
    "    if drop_probability is None:\n",
    "        drop_probability = np.random.uniform(0.05, 0.2)\n",
    "    midi_data = pretty_midi.PrettyMIDI(midi_file)\n",
    "    for instrument in midi_data.instruments:\n",
    "        instrument.notes = [note for note in instrument.notes if np.random.random() >= drop_probability]\n",
    "    return midi_data\n",
    "\n",
    "def preprocess_midi_into_mel_and_mfcc(midi_data, segment_length=224, mfcc_segment_length=519, num_cepstral=13):\n",
    "    audio_data = midi_data.synthesize()\n",
    "    rate = 44100\n",
    "\n",
    "    spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=rate, n_mels=224)\n",
    "    spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "\n",
    "    mfcc_features = mfcc(audio_data, samplerate=rate, numcep=num_cepstral, winlen=0.025, winstep=0.01, nfft=1103)\n",
    "\n",
    "    if spectrogram_db.shape[1] % segment_length != 0:\n",
    "            spectrogram_db = spectrogram_db[:, :-(spectrogram_db.shape[1] % segment_length)]\n",
    "\n",
    "    segments = []\n",
    "\n",
    "    for start in range(0, spectrogram_db.shape[1] - segment_length + 1, segment_length):\n",
    "        mel_segment = spectrogram_db[:, start:start + segment_length]\n",
    "        mfcc_start = mfcc_segment_length * (start // segment_length)\n",
    "        mfcc_end = mfcc_start + mfcc_segment_length\n",
    "        mfcc_segment = mfcc_features[mfcc_start:mfcc_end, :]\n",
    "        # Only consider segments that are long enough\n",
    "        if mfcc_segment.shape[0] == mfcc_segment_length:\n",
    "            segments.append((mel_segment, mfcc_segment))\n",
    "\n",
    "    return segments, audio_data, rate\n",
    "\n",
    "def augment_midi_file(file_path):\n",
    "    functions = [transpose, change_tempo, rhythmic_variation, drop_notes]\n",
    "    augmentation_function = np.random.choice(functions)\n",
    "    augmented_midi = augmentation_function(file_path)\n",
    "    print('Processed transformed file:', file_path)\n",
    "    return augmented_midi\n",
    "\n",
    "def process_directory(dir_to_process, composer, X_mel, X_mfcc, y, audio_time_series_list, sampling_rate_list, sequence_count, TARGET_NUMBER, augment=False):\n",
    "    for file in os.listdir(dir_to_process):\n",
    "        if sequence_count[composer] >= TARGET_NUMBER and augment:\n",
    "            break\n",
    "\n",
    "        if file.endswith('.mid'):\n",
    "            file_path = os.path.join(dir_to_process, file)\n",
    "\n",
    "            # if in augmentation process, randomly transform midi\n",
    "            if augment:\n",
    "                midi_obj = augment_midi_file(file_path)\n",
    "            else:\n",
    "                midi_obj = pretty_midi.PrettyMIDI(file_path)\n",
    "\n",
    "            segments, audio_time_series, rate = preprocess_midi_into_mel_and_mfcc(midi_obj)\n",
    "\n",
    "            if segments is not None:\n",
    "                audio_time_series_list.append(audio_time_series)\n",
    "                sampling_rate_list.append(rate)\n",
    "                for mel_segment, mfcc_segment in segments:\n",
    "                    X_mel.append(mel_segment)\n",
    "                    X_mfcc.append(mfcc_segment)\n",
    "                    y.append(composer)\n",
    "                    sequence_count[composer] += 1\n",
    "                    if sequence_count[composer] % 500 == 0:\n",
    "                        print(sequence_count)\n",
    "\n",
    "\n",
    "def preprocess_data_in_directory(base_dir, subfolder, augment_data=False):\n",
    "    y_augmented = []\n",
    "    X_mel_augmented = []\n",
    "    X_mfcc_augmented = []\n",
    "\n",
    "    # Base without augmentation\n",
    "    y_base = []\n",
    "    X_mel_base = []\n",
    "    X_mfcc_base = []\n",
    "\n",
    "    audio_time_series_list = []\n",
    "    sampling_rate_list = []\n",
    "\n",
    "    TARGET_NUMBER = 4000  # target for augmented dataset\n",
    "\n",
    "    file_dir = base_dir + subfolder\n",
    "    composers = os.listdir(file_dir)\n",
    "    # to show progression\n",
    "    sequence_count = {composer: 0 for composer in composers}\n",
    "\n",
    "    for composer in composers:\n",
    "        y = []\n",
    "        X_mel = []\n",
    "        X_mfcc = []\n",
    "        composer_dir = os.path.join(file_dir, composer)\n",
    "        if os.path.isdir(composer_dir):\n",
    "\n",
    "            # Process each directory\n",
    "            process_directory(composer_dir, composer, X_mel, X_mfcc, y, audio_time_series_list, sampling_rate_list, sequence_count, TARGET_NUMBER)\n",
    "\n",
    "            # Push all original data to _base arrays\n",
    "            X_mel_base.extend(X_mel)\n",
    "            X_mfcc_base.extend(X_mfcc)\n",
    "            y_base.extend(y)\n",
    "\n",
    "            # if we're augmenting data, add extra bartok directory, and randomly transform midis:\n",
    "            if augment_data and subfolder == 'train':\n",
    "                dirs_to_process = [composer_dir]\n",
    "                if composer.lower() == 'bartok':\n",
    "                    extra_dir = os.path.join(base_dir, 'Bartok/')\n",
    "                    process_directory(extra_dir, composer, X_mel, X_mfcc, y, audio_time_series_list, sampling_rate_list, sequence_count, TARGET_NUMBER, augment=False)\n",
    "                    dirs_to_process.append(extra_dir)\n",
    "                # if there are too many sequences just in the base, truncate them to balance the data for the augmented sets\n",
    "                if sequence_count[composer] >= TARGET_NUMBER:\n",
    "                    print('Excess truncated')\n",
    "                    y_augmented.extend(y[:TARGET_NUMBER])\n",
    "                    X_mel_augmented.extend(X_mel[:TARGET_NUMBER])\n",
    "                    X_mfcc_augmented.extend(X_mfcc[:TARGET_NUMBER])\n",
    "                    sequence_count[composer] = TARGET_NUMBER\n",
    "                    print(sequence_count)\n",
    "                else:\n",
    "                    while sequence_count[composer] < TARGET_NUMBER:\n",
    "                        for dir_to_process in dirs_to_process:\n",
    "                            process_directory(dir_to_process, composer, X_mel, X_mfcc, y, audio_time_series_list, sampling_rate_list, sequence_count, TARGET_NUMBER, augment=True)\n",
    "                    y_augmented.extend(y[:TARGET_NUMBER])\n",
    "                    X_mel_augmented.extend(X_mel[:TARGET_NUMBER])\n",
    "                    X_mfcc_augmented.extend(X_mfcc[:TARGET_NUMBER])\n",
    "\n",
    "    audio_time_series_list = [np.array(x) for x in audio_time_series_list]\n",
    "    sampling_rate_list = [np.array(x) for x in sampling_rate_list]\n",
    "\n",
    "    if augment_data:\n",
    "        return np.array(X_mel_augmented), np.array(X_mfcc_augmented), np.array(y_augmented), np.array(X_mel_base), np.array(X_mfcc_base), np.array(y_base), audio_time_series_list, sampling_rate_list\n",
    "    else:\n",
    "        return np.array(X_mel_base), np.array(X_mfcc_base), np.array(y_base), audio_time_series_list, sampling_rate_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_data = pretty_midi.PrettyMIDI(raw_data_folder + 'train/bach/bach342.mid')\n",
    "midi_transposed = transpose(raw_data_folder + 'train/bach/bach342.mid', -5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_transposed.write(raw_data_folder + 'transposed_bach.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file_list = raw_data_folder + 'Bartok/'\n",
    "df_new_files = pd.read_csv(new_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mel_augmented, \\\n",
    "X_train_mfcc_augmented, \\\n",
    "y_train_augmented, \\\n",
    "X_train_mel_original, \\\n",
    "X_train_mfcc_original, \\\n",
    "y_train_original, \\\n",
    "audio_time_series_train, \\\n",
    "sampling_rate_train = preprocess_data_in_directory(raw_data_folder, 'train', augment_data=True)\n",
    "X_test_mel, X_test_mfcc, y_test, audio_time_series_test, sampling_rate_test = preprocess_data_in_directory(raw_data_folder, 'test')\n",
    "X_dev_mel, X_dev_mfcc, y_dev, audio_time_series_dev, sampling_rate_dev = preprocess_data_in_directory(raw_data_folder, 'dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Encoding Composer Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "y_train_original = le.fit_transform(y_train_original)\n",
    "y_train_augmented = le.transform(y_train_augmented)\n",
    "y_test = le.transform(y_test)\n",
    "y_dev = le.transform(y_dev)\n",
    "\n",
    "class_list = le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(raw_data_folder + 'class_list.npy', np.array(class_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Convertin Data into Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mel_augmented = torch.from_numpy(X_train_mel_augmented).float()\n",
    "X_train_mel_augmented = X_train_mel_augmented.unsqueeze(1)\n",
    "\n",
    "X_train_mfcc_augmented = torch.from_numpy(X_train_mfcc_augmented).float()\n",
    "\n",
    "y_train_augmented = torch.from_numpy(y_train_augmented).long()\n",
    "\n",
    "X_test_mel = torch.from_numpy(X_test_mel).float()\n",
    "X_test_mel = X_test_mel.unsqueeze(1)\n",
    "\n",
    "X_test_mfcc = torch.from_numpy(X_test_mfcc).float()\n",
    "\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "\n",
    "X_dev_mel = torch.from_numpy(X_dev_mel).float()\n",
    "X_dev_mel = X_dev_mel.unsqueeze(1)\n",
    "\n",
    "X_dev_mfcc = torch.from_numpy(X_dev_mfcc).float()\n",
    "\n",
    "y_dev_ = torch.from_numpy(y_dev).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Standardizing The Mel Spectrogram Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value_mel = X_train_mel_augmented.min()\n",
    "max_value_mel = X_train_mel_augmented.max()\n",
    "\n",
    "X_train_mel_augmented_norm = (X_train_mel_augmented - min_value_mel) / (max_value_mel - min_value_mel)\n",
    "X_test_mel_normalized = (X_test_mel - min_value_mel) / (max_value_mel - min_value_mel)\n",
    "X_dev_mel_normalized = (X_dev_mel - min_value_mel) / (max_value_mel - min_value_mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Mel Spec data, transform to 3 channels, for use with pretrained models\n",
    "X_train_mel_augmented_norm_rgb = X_train_mel_augmented_norm.repeat(1, 3, 1, 1)\n",
    "X_test_mel_normalized_rgb = X_test_mel_normalized.repeat(1, 3, 1, 1)\n",
    "X_dev_mel_normalized_rgb = X_dev_mel_normalized.repeat(1, 3, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2908,
     "status": "ok",
     "timestamp": 1690236595709,
     "user": {
      "displayName": "Jeffrey Thomas",
      "userId": "00428686483222286344"
     },
     "user_tz": 420
    },
    "id": "XMmpWmiDEfM-"
   },
   "outputs": [],
   "source": [
    "# Saving Results\n",
    "torch.save(X_train_mel_augmented_norm_rgb, raw_data_folder + 'X_train_mel_augmented_norm_rgb.pt')\n",
    "torch.save(X_train_mfcc_augmented, raw_data_folder + 'X_train_mfcc_augmented.pt')\n",
    "torch.save(y_train_augmented, raw_data_folder + 'y_train_augmented.pt')\n",
    "\n",
    "np.save(raw_data_folder + 'audio_time_series_train.npy', audio_time_series_train)\n",
    "np.save(raw_data_folder + 'sampling_rate_train.npy', sampling_rate_train)\n",
    "\n",
    "torch.save(X_test_mel_normalized_rgb, raw_data_folder + 'X_test_mel_normalized_rgb.pt')\n",
    "torch.save(X_test_mfcc, raw_data_folder + 'X_test_mfcc.pt')\n",
    "torch.save(y_test, raw_data_folder + 'y_test.pt')\n",
    "\n",
    "np.save(raw_data_folder + 'audio_time_series_test.npy', audio_time_series_test)\n",
    "np.save(raw_data_folder + 'sampling_rate_test.npy', sampling_rate_test)\n",
    "\n",
    "torch.save(X_dev_mel_normalized_rgb, raw_data_folder + 'X_dev_mel_normalized_rgb.pt')\n",
    "torch.save(X_dev_mfcc, raw_data_folder + 'X_dev_mfcc.pt')\n",
    "torch.save(y_dev, raw_data_folder + 'y_dev.pt')\n",
    "\n",
    "np.save(raw_data_folder + 'audio_time_series_dev.npy', audio_time_series_dev)\n",
    "np.save(raw_data_folder + 'sampling_rate_dev.npy', sampling_rate_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the preproccessing section the MFCC and Mel data was extracted from audio files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder = 'Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 2122,
     "status": "ok",
     "timestamp": 1690349322619,
     "user": {
      "displayName": "Jeffrey Thomas",
      "userId": "00428686483222286344"
     },
     "user_tz": 420
    },
    "id": "rsb3sZoIFSCw"
   },
   "outputs": [],
   "source": [
    "X_train_rgb = torch.load(raw_data_folder + 'X_train_rgb.pt')\n",
    "X_train_mfcc_denorm = torch.load(raw_data_folder + 'X_train_mfcc_denorm.pt')\n",
    "y_train = torch.load(raw_data_folder + 'y_train_rgb.pt')\n",
    "audio_time_series_train = np.load(raw_data_folder + 'audio_time_series_train.npy', allow_pickle=True)\n",
    "sampling_rate_train = np.load(raw_data_folder + 'sampling_rate_train.npy', allow_pickle=True)\n",
    "\n",
    "X_test_rgb = torch.load(raw_data_folder + 'X_test_rgb.pt')\n",
    "X_test_mfcc_denorm = torch.load(raw_data_folder + 'X_test_mfcc_denorm.pt')\n",
    "y_test = torch.load(raw_data_folder + 'y_test_rgb.pt')\n",
    "audio_time_series_test = np.load(raw_data_folder + 'audio_time_series_test.npy', allow_pickle=True)\n",
    "sampling_rate_test = np.load(raw_data_folder + 'sampling_rate_test.npy', allow_pickle=True)\n",
    "\n",
    "X_dev_rgb = torch.load(raw_data_folder + 'X_dev_rgb.pt')\n",
    "X_dev_mfcc_denorm = torch.load(raw_data_folder + 'X_dev_mfcc_denorm.pt')\n",
    "y_dev = torch.load(raw_data_folder + 'y_dev_rgb.pt')\n",
    "audio_time_series_dev = np.load(raw_data_folder + 'audio_time_series_dev.npy', allow_pickle=True)\n",
    "sampling_rate_dev = np.load(raw_data_folder + 'sampling_rate_dev.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = np.load(raw_data_folder + 'class_list.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Counting Number of Unique Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique values and their counts\n",
    "unique_values, counts = torch.unique(y_train_augmented, return_counts=True)\n",
    "\n",
    "# Print unique values and their counts\n",
    "for value, count in zip(unique_values, counts):\n",
    "    print(f'{value}: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Visualizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# accumulate composer names for visualizations\n",
    "composer_names = []\n",
    "composers = os.listdir(raw_data_folder + 'train')\n",
    "for composer in composers:\n",
    "    composer_dir = os.path.join(raw_data_folder + 'train', composer)\n",
    "    if os.path.isdir(composer_dir):\n",
    "        for file in os.listdir(composer_dir):\n",
    "            if file.endswith('.mid'):\n",
    "                file_path = os.path.join(composer_dir, file)\n",
    "                composer_names.append(composer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Mel Spectrogram's\n",
    "fig, axs = plt.subplots(2, figsize=(10, 8))\n",
    "\n",
    "# denormalize the first example\n",
    "X_train_denorm1 = X_train_rgb[1220][0].cpu() * (max_value_mel - min_value_mel) + min_value_mel\n",
    "\n",
    "# plot the first example\n",
    "im1 = axs[0].imshow(X_train_denorm1, cmap='jet', aspect='auto', origin='lower')\n",
    "axs[0].set_title(f'Mel spectrogram, {class_list[y_train[1220]]}')\n",
    "fig.colorbar(im1, ax=axs[0], format='%+2.0f dB')\n",
    "\n",
    "# denormalize the second example\n",
    "X_train_denorm2 = X_train_rgb[13450][0].cpu() * (max_value_mel - min_value_mel) + min_value_mel\n",
    "\n",
    "# plot the second example\n",
    "im2 = axs[1].imshow(X_train_denorm2, cmap='jet', aspect='auto', origin='lower')\n",
    "axs[1].set_title(f'Mel spectrogram, {class_list[y_train[13450]]}')\n",
    "fig.colorbar(im2, ax=axs[1], format='%+2.0f dB')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Example MFCC's\n",
    "fig, axs = plt.subplots(2, figsize=(10, 8))\n",
    "im1 = axs[0].imshow(X_train_mfcc[1220][0].cpu(), cmap='jet', aspect='auto', origin='lower')\n",
    "axs[0].set_title(f'MFCC, {class_list[y_train[1220]]}')\n",
    "\n",
    "im2 = axs[1].imshow(X_train_mfcc[13450][0].cpu(), cmap='jet', aspect='auto', origin='lower')\n",
    "axs[1].set_title(f'MFCC, {class_list[y_train[13450]]}')\n",
    "fig.colorbar(im2, ax=axs[1], format='%+2.0f dB')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_energy(mel_spectrogram):\n",
    "    return torch.sum(mel_spectrogram**2, axis=1)\n",
    "\n",
    "# Convert y_train to a numpy array\n",
    "y_train_np = np.array(y_train.cpu())\n",
    "\n",
    "# Compute the mean energy for each Mel-spectrogram in training set\n",
    "mean_energies = [torch.mean(compute_energy(mel_spectrogram)).item() for mel_spectrogram in X_train_mel]\n",
    "\n",
    "composer_names = [class_list[i] for i in y_train_np]\n",
    "data = pd.DataFrame({'Composer': composer_names, 'Mean Energy': mean_energies})\n",
    "\n",
    "# Group by composer and compute the mean of the mean energies\n",
    "grouped = data.groupby('Composer')['Mean Energy'].mean()\n",
    "\n",
    "# Plot a bar chart of the mean energies by composer\n",
    "grouped.plot(kind='bar')\n",
    "plt.ylabel('Mean Energy')\n",
    "plt.title('Mean Energy by Composer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_spectral_contrast(mel_spectrogram):\n",
    "    # Convert the tensor to a numpy array and compute the spectral contrast\n",
    "    mel_spectrogram_np = mel_spectrogram.cpu().numpy()\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(S=mel_spectrogram_np)\n",
    "    return np.mean(spectral_contrast)\n",
    "\n",
    "# Compute the mean spectral contrast for each Mel-spectrogram in training set\n",
    "mean_spectral_contrasts = [compute_spectral_contrast(mel_spectrogram) for mel_spectrogram in X_train_mel]\n",
    "\n",
    "# Map integer labels to composer names\n",
    "composer_names = [class_list[i] for i in y_train_np]\n",
    "\n",
    "# Combine the composers and mean spectral contrasts into a DataFrame\n",
    "data = pd.DataFrame({'Composer': composer_names, 'Mean Spectral Contrast': mean_spectral_contrasts})\n",
    "\n",
    "# Group by composer and compute the mean of the mean spectral contrasts\n",
    "grouped = data.groupby('Composer')['Mean Spectral Contrast'].mean()\n",
    "\n",
    "# Plot a bar chart of the mean spectral contrasts by composer\n",
    "grouped.plot(kind='bar')\n",
    "plt.ylabel('Mean Spectral Contrast')\n",
    "plt.title('Mean Spectral Contrast by Composer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Calculate chroma features for each audio file\n",
    "chroma_features = [librosa.feature.chroma_stft(y=audio_time_series, sr=sampling_rate) for audio_time_series, sampling_rate in zip(audio_time_series_train, sampling_rate_train)]\n",
    "\n",
    "# Compute the mean chroma feature for each audio file\n",
    "mean_chroma_features = [np.mean(chroma_feature, axis=1) for chroma_feature in chroma_features]\n",
    "\n",
    "# Convert the list of mean chroma features to a 2D array\n",
    "mean_chroma_features_array = np.array(mean_chroma_features)\n",
    "\n",
    "# Combine the composers and mean chroma features into a DataFrame\n",
    "data = pd.DataFrame(mean_chroma_features_array, columns=[f'Chroma {i+1}' for i in range(12)])\n",
    "data['Composer'] = composer_names\n",
    "\n",
    "# Melt the DataFrame to long format for plotting\n",
    "data_melted = pd.melt(data, id_vars='Composer', var_name='Chroma', value_name='Mean Chroma Feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot a boxplot of the mean chroma features by composer\n",
    "plt.figure(figsize=(15,8))\n",
    "ax = sns.boxplot(x='Composer', y='Mean Chroma Feature', hue='Chroma', data=data_melted)\n",
    "ax.legend_.remove()\n",
    "plt.title('Mean Chroma Features by Composer')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Weighted Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1690349325584,
     "user": {
      "displayName": "Jeffrey Thomas",
      "userId": "00428686483222286344"
     },
     "user_tz": 420
    },
    "id": "1XpHl97poGMg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 234\n",
      "1: 111\n",
      "2: 194\n",
      "3: 221\n",
      "4: 197\n",
      "5: 500\n",
      "6: 227\n",
      "7: 425\n",
      "8: 265\n"
     ]
    }
   ],
   "source": [
    "# Get unique values and their counts\n",
    "unique_values, counts = torch.unique(y_train.cpu(), return_counts=True)\n",
    "\n",
    "# Print unique values and their counts\n",
    "for value, count in zip(unique_values, counts):\n",
    "    print(f'{value}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2374"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting Class Weights\n",
    "target = torch.Tensor.numpy(y_train.cpu())\n",
    "target = target.astype('int')\n",
    "class_sample_counts = np.array([len(np.where(target == t)[0]) for t in np.unique(target)])\n",
    "class_weights = [1/class_sample_counts[i] for i in target]\n",
    "len(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = WeightedRandomSampler(weights = class_weights, num_samples = len(class_weights), replacement = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Making Mel DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelDataset(Dataset):\n",
    "    def __init__(self, X_mel, y, class_list):\n",
    "        self.X_mel = X_mel\n",
    "        self.y = y\n",
    "        self.class_list = class_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_mel)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_mel[idx], self.y[idx]\n",
    "\n",
    "    def classes(self):\n",
    "        # Return the list of class labels\n",
    "        return self.class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_mel = MelDataset(X_train_mel, y_train, class_list)\n",
    "test_dataset_mel = MelDataset(X_test_mel, y_test, class_list)\n",
    "val_dataset_mel = MelDataset(X_dev_mel, y_dev, class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Weighted Sampling\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader\n",
    "train_loader_mel = DataLoader(train_dataset_mel, batch_size=batch_size, shuffle=True)\n",
    "test_loader_mel = DataLoader(test_dataset_mel, batch_size=batch_size, shuffle=False)\n",
    "val_loader_mel = DataLoader(val_dataset_mel, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Weighted Sampling\n",
    "weighted_train_loader_mel = DataLoader(train_dataset_mel, sampler = sampler,  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Making MFCC Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCCDataset(Dataset):\n",
    "    def __init__(self, X_mfcc, y, class_list):\n",
    "        self.X_mfcc = X_mfcc\n",
    "        self.y = y\n",
    "        self.class_list = class_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_mfcc)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_mfcc[idx], self.y[idx]\n",
    "\n",
    "    def classes(self):\n",
    "        # Return the list of class labels\n",
    "        return self.class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_mfcc = MFCCDataset(X_train_mfcc, y_train, class_list) \n",
    "test_dataset_mfcc = MFCCDataset(X_test_mfcc, y_test, class_list) \n",
    "val_dataset_mfcc = MFCCDataset(X_dev_mfcc, y_dev, class_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Weighted Sampling\n",
    "train_loader_mfcc = DataLoader(train_dataset_mfcc, batch_size=batch_size, shuffle=True)\n",
    "test_loader_mfcc = DataLoader(test_dataset_mfcc, batch_size=batch_size, shuffle=False)\n",
    "val_loader_mfcc = DataLoader(val_dataset_mfcc, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Weighted Sampling\n",
    "weighted_train_loader_mfcc = DataLoader(train_dataset_mfcc, sampler = sampler,  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Making MEL and MFCC Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, X_mel, X_mfcc, y, class_list):\n",
    "        self.X_mel = X_mel\n",
    "        self.X_mfcc = X_mfcc\n",
    "        self.y = y\n",
    "        self.class_list = class_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_mfcc)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_mel[idx], self.X_mfcc[idx], self.y[idx]\n",
    "\n",
    "    def classes(self):\n",
    "        # Return the list of class labels\n",
    "        return self.class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_combined = CombinedDataset(X_train_mel, X_train_mfcc, y_train, class_list)\n",
    "test_dataset_combined = CombinedDataset(X_test_mel, X_test_mfcc, y_test, class_list)\n",
    "val_dataset_combined = CombinedDataset(X_dev_mel, X_dev_mfcc, y_dev, class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Weighted Sampling\n",
    "train_loader_combined = DataLoader(train_dataset_combined, batch_size=batch_size, shuffle=True)\n",
    "test_loader_combined = DataLoader(test_dataset_combined, batch_size=batch_size, shuffle=False)\n",
    "val_loader_combined = DataLoader(val_dataset_combined, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Weighted Sampling\n",
    "weighted_train_loader_combined = DataLoader(train_dataset_combined, sampler = sampler,  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MEL_CNN(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, init_filters, num_classes, dropout_rate=0.2):\n",
    "        super(MEL_CNN, self).__init__()\n",
    "        self.hidden_layers = num_hidden_layers\n",
    "        \n",
    "        #Pooling Layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Input Layer\n",
    "        self.layers = nn.ModuleDict()\n",
    "        self.layers[\"input_Conv\"] = nn.Conv2d(3, init_filters, kernel_size=3, padding=1)\n",
    "        self.layers[\"input_batch_norm\"] = nn.BatchNorm2d(init_filters)\n",
    "        \n",
    "        # Hidden Layers\n",
    "        for i in range(num_hidden_layers):\n",
    "            in_channels = 2 ** i * init_filters\n",
    "            out_channels = in_channels * 2\n",
    "            self.layers[f\"hidden_{i}\"] = nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1)\n",
    "            self.layers[f\"batch_norm_{i}\"] = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Last Layer\n",
    "        final_in_channels = 2 ** num_hidden_layers * init_filters\n",
    "        final_out_channels = final_in_channels * 4\n",
    "        self.layers[\"output_Conv\"] = nn.Conv2d(final_in_channels, final_out_channels, kernel_size = 1)\n",
    "        self.layers[\"output_batch_norm\"] = nn.BatchNorm2d(final_out_channels)\n",
    "        self.layers[\"linear\"] = nn.Linear(final_out_channels, 128)\n",
    "        self.layers[\"dropout\"] = nn.Dropout(p = dropout_rate)\n",
    "        self.layers[\"linear_output_1\"] = nn.Linear(128, 64)\n",
    "        self.layers[\"linear_output_2\"] = nn.Linear(64, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.layers[\"input_batch_norm\"](self.layers[\"input_Conv\"](x))))\n",
    "        for i in range(self.hidden_layers):\n",
    "            x = self.pool(nn.functional.relu(self.layers[f\"batch_norm_{i}\"](self.layers[f\"hidden_{i}\"](x))))\n",
    "            \n",
    "        x = self.pool(nn.functional.relu(self.layers[\"output_batch_norm\"](self.layers[\"output_Conv\"](x))))\n",
    "        x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = nn.functional.relu(self.layers[\"linear\"](x))\n",
    "        x = self.layers[\"dropout\"](x)\n",
    "        x = nn.functional.relu(self.layers[\"linear_output_1\"](x))\n",
    "        x = self.layers[\"linear_output_2\"](x)\n",
    "        return x # Don't need softmax in the forward function since we're using cross-entropy loss, which implicitly performs softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCC_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(MFCC_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(x.device)  # Multiply by 2 because it's bidirectional\n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(x.device)  # Multiply by 2 because it's bidirectional\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # shape = (batch_size, seq_length, hidden_size*2)\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM_Ensemble(nn.Module):\n",
    "    def __init__(self, model_CNN, model_LSTM, num_class):\n",
    "        super(CNN_LSTM_Ensemble, self).__init__()\n",
    "        self.LSTM = model_LSTM\n",
    "        self.CNN = model_CNN\n",
    "\n",
    "        fc_input_size = model_CNN.final_out_channel + model_LSTM.hidden_size*2\n",
    "        \n",
    "        # Define additional fully-connected layers\n",
    "        self.fc1 = nn.Linear(fc_input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "    \n",
    "    def forward(self, MEL, MFCC):\n",
    "        output_CNN = self.CNN(MEL)\n",
    "        output_LSTM = self.LSTM(MFCC)\n",
    "        output = torch.cat((output_CNN, output_LSTM), dim = 1)\n",
    "        \n",
    "        output = nn.functional.relu(self.fc1(output))\n",
    "        output = nn.functional.relu(self.fc2(output))\n",
    "        output = self.fc3(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, scheduler, train_loader, test_loader, ensemble=False, num_epochs=100, patience=10, model_id = \"Best_Model.pt\"):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #Training Results\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    train_roc = []\n",
    "    train_f1 = []\n",
    "    # Testing Results\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    test_roc = []\n",
    "    test_f1 = []\n",
    "    #Patience\n",
    "    best_test_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        train_acc_metric = MulticlassAccuracy()\n",
    "        train_roc_metric = MulticlassAUROC()\n",
    "        train_f1_metric = MulticlassF1Score()\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            if ensemble:\n",
    "                mel_inputs, mfcc_inputs, labels = batch\n",
    "                mel_inputs, mfcc_inputs, labels = mel_inputs.to(device), mfcc_inputs.to(device), labels.to(device)\n",
    "                outputs = model(mel_inputs, mfcc_inputs)\n",
    "            else:\n",
    "                inputs, labels = batch\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Update Metrics\n",
    "            train_acc_metric.update(outputs, labels)\n",
    "            train_roc_metric.update(outputs, labels)\n",
    "            train_f1_metric.update(outputs, labels)\n",
    "\n",
    "            '''\n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            labels = labels.long()\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            '''\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * mel_inputs.size(0)\n",
    "\n",
    "        # Compute Metrics\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_accuracy = train_acc_metric.compute(outputs, labels)\n",
    "        train_single_roc = train_roc_metric.compute(outputs, labels)\n",
    "        train_single_f1 = train_f1_metric.compute(outputs, labels)\n",
    "        \n",
    "        # Append Results\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        train_roc.append(train_single_roc)\n",
    "        train_f1.append(train_single_f1)\n",
    "        \n",
    "        \n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "\n",
    "        # Evaluation\n",
    "        test_acc_metric = MulticlassAccuracy()\n",
    "        test_roc_metric = MulticlassAUROC()\n",
    "        test_f1_metric = MulticlassF1Score()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0.0\n",
    "            test_correct = 0\n",
    "            test_total = 0\n",
    "\n",
    "            for batch in test_loader:\n",
    "                if ensemble:\n",
    "                    mel_inputs, mfcc_inputs, labels = batch\n",
    "                    mel_inputs, mfcc_inputs, labels = mel_inputs.to(device), mfcc_inputs.to(device), labels.to(device)\n",
    "                    outputs = model(mel_inputs, mfcc_inputs)\n",
    "                else:\n",
    "                    inputs, labels = batch\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    \n",
    "                '''\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                '''\n",
    "                # Update Metrics\n",
    "                train_acc_metric.update(outputs, labels)\n",
    "                train_roc_metric.update(outputs, labels)\n",
    "                train_f1_metric.update(outputs, labels)\n",
    "\n",
    "                '''\n",
    "                # Compute accuracy and loss\n",
    "                test_loss += criterion(outputs, labels).item() * mel_inputs.size(0)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "                '''\n",
    "            # Compute Metrics\n",
    "            test_loss /= len(test_loader.dataset)\n",
    "            test_accuracy = test_acc_metric.compute(outputs, labels)\n",
    "            test_single_roc = test_roc_metric.compute(outputs, labels)\n",
    "            test_single_f1 = test_f1_metric.compute(outputs, labels)\n",
    "            \n",
    "            # Append Results\n",
    "            test_losses.append(test_loss)\n",
    "            test_accuracies.append(test_accuracy)\n",
    "            test_roc.append(test_single_roc)\n",
    "            test_f1.append(test_single_f1)\n",
    "    \n",
    "        print(f'Epoch {epoch}, train loss: {train_loss:.4f}, train accuracy: {train_accuracy:.2f}, test loss: {test_loss:.4f}, test accuracy: {test_accuracy:.2f}, learning rate: {scheduler.get_lr()[0]}')\n",
    "\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            epochs_no_improve = 0\n",
    "            print('Saving model!')\n",
    "            torch.save(model.state_dict(), model_id)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve in [20, 30, 40]:\n",
    "                print('Stepping lr_scheduler')\n",
    "                scheduler.step()\n",
    "            if epochs_no_improve == patience:\n",
    "                print('Early stopping!')\n",
    "                break\n",
    "    results = {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'train_AUCROC': train_roc,\n",
    "        'train_F1': train_f1,\n",
    "        'test_losses': test_losses,\n",
    "        'test_accuracies': test_accuracies,\n",
    "        'test_AUCROC': test_roc,\n",
    "        'test_F1': test_f1,\n",
    "        'best_test_loss': best_test_loss\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (layers): ModuleDict(\n",
      "    (input_Conv): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (input_batch_norm): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (hidden_0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batch_norm_0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (hidden_1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (output_Conv): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (output_batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (linear_output_1): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (linear_output_2): Linear(in_features=64, out_features=9, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "hl = 2\n",
    "init = 8\n",
    "num_classes = len(torch.unique(y_train.cpu()))\n",
    "dropout = 0.5\n",
    "test_model = MEL_CNN(hl, init, num_classes, dropout)\n",
    "\n",
    "print(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 10\n",
    "num_epochs = 50\n",
    "lr = 0.001\n",
    "weight_decay=0.005\n",
    "optimizer = torch.optim.Adam(test_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "test_model, optimizer, weighted_train_loader_mel, test_loader_mel = accelerator.prepare(\n",
    "test_model, optimizer, weighted_train_loader_mel, test_loader_mel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 1.9713, train accuracy: 0.40, test loss: 2.0165, test accuracy: 0.38, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 1, train loss: 1.9622, train accuracy: 0.41, test loss: 2.0151, test accuracy: 0.34, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 2, train loss: 2.0005, train accuracy: 0.36, test loss: 2.0101, test accuracy: 0.35, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 3, train loss: 1.9742, train accuracy: 0.40, test loss: 1.9875, test accuracy: 0.37, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 4, train loss: 1.9774, train accuracy: 0.40, test loss: 1.9772, test accuracy: 0.40, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 5, train loss: 1.9762, train accuracy: 0.40, test loss: 1.9916, test accuracy: 0.38, learning rate: 0.001\n",
      "Epoch 6, train loss: 1.9750, train accuracy: 0.40, test loss: 1.9766, test accuracy: 0.40, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 7, train loss: 1.9761, train accuracy: 0.40, test loss: 1.9733, test accuracy: 0.41, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 8, train loss: 1.9836, train accuracy: 0.39, test loss: 1.9835, test accuracy: 0.40, learning rate: 0.001\n",
      "Epoch 9, train loss: 1.9752, train accuracy: 0.40, test loss: 2.1336, test accuracy: 0.18, learning rate: 0.001\n",
      "Epoch 10, train loss: 1.9697, train accuracy: 0.41, test loss: 1.9695, test accuracy: 0.44, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 11, train loss: 1.9670, train accuracy: 0.41, test loss: 2.0757, test accuracy: 0.32, learning rate: 0.001\n",
      "Epoch 12, train loss: 1.9605, train accuracy: 0.42, test loss: 2.0627, test accuracy: 0.27, learning rate: 0.001\n",
      "Epoch 13, train loss: 1.9639, train accuracy: 0.41, test loss: 1.9993, test accuracy: 0.35, learning rate: 0.001\n",
      "Epoch 14, train loss: 1.9659, train accuracy: 0.40, test loss: 1.9719, test accuracy: 0.39, learning rate: 0.001\n",
      "Epoch 15, train loss: 1.9512, train accuracy: 0.43, test loss: 1.9752, test accuracy: 0.39, learning rate: 0.001\n",
      "Epoch 16, train loss: 1.9625, train accuracy: 0.41, test loss: 2.0049, test accuracy: 0.36, learning rate: 0.001\n",
      "Epoch 17, train loss: 1.9614, train accuracy: 0.42, test loss: 2.0183, test accuracy: 0.35, learning rate: 0.001\n",
      "Epoch 18, train loss: 1.9633, train accuracy: 0.41, test loss: 2.0179, test accuracy: 0.36, learning rate: 0.001\n",
      "Epoch 19, train loss: 1.9582, train accuracy: 0.43, test loss: 2.0728, test accuracy: 0.27, learning rate: 0.001\n",
      "Epoch 20, train loss: 1.9670, train accuracy: 0.41, test loss: 1.9920, test accuracy: 0.40, learning rate: 0.001\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "cnn_results = train_model(test_model, optimizer, scheduler, weighted_train_loader_mel, test_loader_mel, ensemble=False, num_epochs=num_epochs, patience=patience, model_id=\"Best_MEL_CNN.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCC_LSTM(\n",
      "  (lstm): LSTM(13, 32, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=64, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 13\n",
    "hidden_size = 32\n",
    "num_layers = 2\n",
    "num_classes = len(torch.unique(y_train.cpu()))\n",
    "\n",
    "lstm_test_model = MFCC_LSTM(input_size, hidden_size, num_layers, num_classes)\n",
    "\n",
    "print(lstm_test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 10\n",
    "num_epochs = 50\n",
    "lr = 0.001\n",
    "weight_decay = 0.01\n",
    "\n",
    "optimizer = torch.optim.Adam(lstm_test_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "lstm_test_model, optimizer, weighted_train_loader_mfcc, test_loader_mfcc = accelerator.prepare(\n",
    "lstm_test_model, optimizer, weighted_train_loader_mfcc, test_loader_mfcc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 2.1945, train accuracy: 0.14, test loss: 2.1974, test accuracy: 0.11, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 1, train loss: 2.1787, train accuracy: 0.17, test loss: 2.1882, test accuracy: 0.16, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 2, train loss: 2.1482, train accuracy: 0.17, test loss: 2.1825, test accuracy: 0.13, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 3, train loss: 2.1066, train accuracy: 0.18, test loss: 2.1300, test accuracy: 0.15, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 4, train loss: 2.1332, train accuracy: 0.16, test loss: 2.1464, test accuracy: 0.10, learning rate: 0.001\n",
      "Epoch 5, train loss: 2.0835, train accuracy: 0.19, test loss: 2.1280, test accuracy: 0.10, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 6, train loss: 2.0502, train accuracy: 0.20, test loss: 2.1192, test accuracy: 0.13, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 7, train loss: 2.1069, train accuracy: 0.17, test loss: 2.1305, test accuracy: 0.14, learning rate: 0.001\n",
      "Epoch 8, train loss: 2.0446, train accuracy: 0.21, test loss: 2.1229, test accuracy: 0.12, learning rate: 0.001\n",
      "Epoch 9, train loss: 2.0389, train accuracy: 0.21, test loss: 2.1471, test accuracy: 0.08, learning rate: 0.001\n",
      "Epoch 10, train loss: 2.1089, train accuracy: 0.17, test loss: 2.1478, test accuracy: 0.15, learning rate: 0.001\n",
      "Epoch 11, train loss: 2.0407, train accuracy: 0.19, test loss: 1.9792, test accuracy: 0.18, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 12, train loss: 1.9878, train accuracy: 0.19, test loss: 1.9382, test accuracy: 0.17, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 13, train loss: 1.9396, train accuracy: 0.21, test loss: 2.1063, test accuracy: 0.16, learning rate: 0.001\n",
      "Epoch 14, train loss: 2.1065, train accuracy: 0.16, test loss: 2.1168, test accuracy: 0.11, learning rate: 0.001\n",
      "Epoch 15, train loss: 2.0401, train accuracy: 0.19, test loss: 2.0872, test accuracy: 0.11, learning rate: 0.001\n",
      "Epoch 16, train loss: 1.9818, train accuracy: 0.19, test loss: 2.0662, test accuracy: 0.11, learning rate: 0.001\n",
      "Epoch 17, train loss: 2.0847, train accuracy: 0.20, test loss: 2.0351, test accuracy: 0.13, learning rate: 0.001\n",
      "Epoch 18, train loss: 1.9933, train accuracy: 0.22, test loss: 1.9916, test accuracy: 0.15, learning rate: 0.001\n",
      "Epoch 19, train loss: 1.9690, train accuracy: 0.23, test loss: 1.9850, test accuracy: 0.16, learning rate: 0.001\n",
      "Epoch 20, train loss: 1.9838, train accuracy: 0.22, test loss: 2.0107, test accuracy: 0.16, learning rate: 0.001\n",
      "Epoch 21, train loss: 1.9691, train accuracy: 0.23, test loss: 1.9560, test accuracy: 0.13, learning rate: 0.001\n",
      "Epoch 22, train loss: 1.9555, train accuracy: 0.22, test loss: 1.9823, test accuracy: 0.13, learning rate: 0.001\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "lstm_results = train_model(lstm_test_model, optimizer, scheduler, weighted_train_loader_mfcc, test_loader_mfcc, ensemble=False, num_epochs=num_epochs, patience=patience, model_id=\"Best_MFCC_LSTM.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing Ensemble Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEL_CNN(\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (layers): ModuleDict(\n",
      "    (input_Conv): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (input_batch_norm): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (hidden_0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batch_norm_0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (hidden_1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (output_Conv): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (output_batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (linear_output_1): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (linear_output_2): Linear(in_features=64, out_features=9, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Mel Model\n",
    "hl = 2\n",
    "init = 8\n",
    "num_classes = len(torch.unique(y_train.cpu()))\n",
    "dropout = 0.5\n",
    "mel_model = MEL_CNN(hl, init, num_classes, dropout)\n",
    "\n",
    "print(mel_model)\n",
    "\n",
    "mel_model = mel_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCC_LSTM(\n",
      "  (lstm): LSTM(13, 32, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=64, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# MFCC model\n",
    "input_size = 13\n",
    "hidden_size = 32\n",
    "num_layers = 2\n",
    "num_classes = len(torch.unique(y_train.cpu()))\n",
    "\n",
    "mfcc_model = MFCC_LSTM(input_size, hidden_size, num_layers, num_classes)\n",
    "\n",
    "print(mfcc_model)\n",
    "mfcc_model = mfcc_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_LSTM_Ensemble(\n",
      "  (LSTM): MFCC_LSTM(\n",
      "    (lstm): LSTM(13, 32, num_layers=2, batch_first=True, bidirectional=True)\n",
      "    (fc): Linear(in_features=64, out_features=9, bias=True)\n",
      "  )\n",
      "  (CNN): MEL_CNN(\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (layers): ModuleDict(\n",
      "      (input_Conv): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (input_batch_norm): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (hidden_0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (batch_norm_0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (hidden_1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (output_Conv): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (output_batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "      (linear_output_1): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (linear_output_2): Linear(in_features=64, out_features=9, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (outward): Linear(in_features=9, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Ensemble\n",
    "test_ensemble = CNN_LSTM_Ensemble(mel_model, mfcc_model, num_classes)\n",
    "print(test_ensemble)\n",
    "test_ensemble = test_ensemble.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 10\n",
    "num_epochs = 50\n",
    "lr = 0.001\n",
    "weight_decay = 0.01\n",
    "\n",
    "optimizer = torch.optim.Adam(test_ensemble.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "test_ensemble, optimizer, weighted_train_loader_combined, test_loader_combined = accelerator.prepare(\n",
    "test_ensemble, optimizer, weighted_train_loader_combined, test_loader_combined\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 2.1979, train accuracy: 0.10, test loss: 2.1982, test accuracy: 0.05, learning rate: 0.001\n",
      "Saving model!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[122], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_losses, test_losses, train_acc, test_acc, best_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ensemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_ensemble\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweighted_train_loader_combined\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader_combined\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[104], line 81\u001b[0m, in \u001b[0;36mtrain_ensemble\u001b[1;34m(model, optimizer, scheduler, train_loader, test_loader, num_epochs, patience)\u001b[0m\n\u001b[0;32m     79\u001b[0m     epochs_no_improve \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSaving model!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 81\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[43mmodel_id\u001b[49m)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     83\u001b[0m     epochs_no_improve \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_id' is not defined"
     ]
    }
   ],
   "source": [
    "ensemble_results = train_model(test_ensemble, optimizer, scheduler, weighted_train_loader_combined, test_loader_combined, ensemble=True, num_epochs=num_epochs, patience=patience, model_id=\"Best_Ensemble.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(model, test_loader, ensemble=False):\n",
    "    model.eval()\n",
    "    acc_metric = MulticlassAccuracy()\n",
    "    roc_metric = MulticlassAUROC()\n",
    "    f1_metric = MulticlassF1Score()\n",
    "    results = {}\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            if ensemble:\n",
    "                mel_inputs, mfcc_inputs, labels = batch\n",
    "                mel_inputs = mel_inputs.to(device)  # Move inputs to the device\n",
    "                mfcc_inputs = mfcc_inputs.to(device)\n",
    "                labels = labels.to(device)  # Move labels to the device\n",
    "                outputs = model(mel_inputs, mfcc_inputs)\n",
    "            else:\n",
    "                inputs, labels = batch\n",
    "                inputs = inputs.to(device)  # Move inputs to the device\n",
    "                labels = labels.to(device)  # Move labels to the device\n",
    "                outputs = model(inputs)\n",
    "            '''\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            '''\n",
    "            # Update Metrics\n",
    "            acc_metric.update(ouputs, labels)\n",
    "            roc_metric.update(outputs, labels)\n",
    "            f1_metric.update(outputs, labels)\n",
    "    \n",
    "    acc = acc_metric.compute()\n",
    "    roc = roc_metric.compute()\n",
    "    f1 = f1_metric.compute()\n",
    "    results = {'Accuracy': acc, 'AUC-ROC': roc, 'F1': f1}\n",
    "    print(\"Accuracy %f, AUC-ROC %f, F1 %f\",  % (acc, roc, f1))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cnn_val_accuracy = calculate_accuracy(cnn_model, val_loader_mel)\n",
    "print(f'CNN Validation Accuracy: {cnn_val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "lstm_val_accuracy = calculate_accuracy(lstm_model, val_loader_mfcc)\n",
    "print(f'LSMT Validation Accuracy: {lstm_val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ensemble_val_accuracy = calculate_accuracy(ensemble_model, val_loader_combined, ensemble=True)\n",
    "print(f'Ensemble Validation Accuracy: {ensemble_val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(model, test_loader, num_classes):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for mel_inputs, labels in test_loader:\n",
    "            mel_inputs = mel_inputs.to(device)  # Move inputs to the device\n",
    "            labels = labels.to(device)  # Move labels to the device\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(mel_inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Create the confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions, labels=np.arange(num_classes))\n",
    "    return cm\n",
    "\n",
    "def plot_confusion_matrix(confusion_mtx, labels):\n",
    "    # Normalize the confusion matrix\n",
    "    confusion_mtx = confusion_mtx.astype('float') / confusion_mtx.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    # Create a heatmap using seaborn\n",
    "    sns.heatmap(confusion_mtx, annot=True, fmt='.2f', cmap='Blues', xticklabels=labels, yticklabels=labels, ax=ax)\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "\n",
    "    # Rotate x-axis labels if needed\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(torch.unique(y_dev))\n",
    "confusion_mtx = create_confusion_matrix(model, val_loader, num_classes)\n",
    "class_labels = val_loader.dataset.classes()\n",
    "plot_confusion_matrix(confusion_mtx, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader_mel:\n",
    "        inputs = inputs.to(device)  # Move inputs to the device (e.g., GPU)\n",
    "        labels = labels.to(device)  # Move labels to the device\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = mel_model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Update counts\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader_mel:\n",
    "        inputs = inputs.to(device)  # Move inputs to the device (e.g., GPU)\n",
    "        labels = labels.to(device)  # Move labels to the device\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = mel_model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_predictions, labels=np.arange(num_classes))\n",
    "\n",
    "# Normalize the confusion matrix\n",
    "confusion_mtx = confusion_mtx.astype('float') / confusion_mtx.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "\n",
    "class_labels = val_loader.dataset.classes()\n",
    "# Create a heatmap using seaborn\n",
    "sns.heatmap(confusion_mtx, annot=True, fmt='.2f', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels, ax=ax)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels')\n",
    "ax.set_title('Confusion Matrix')\n",
    "\n",
    "# Rotate x-axis labels if needed\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOUJWzLCGobAI546zASPdEC",
   "mount_file_id": "1BRNgN_JKDJfHFffqSAc_mckufzMmrrpO",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
