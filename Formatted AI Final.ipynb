{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2640,
     "status": "ok",
     "timestamp": 1690349236460,
     "user": {
      "displayName": "Jeffrey Thomas",
      "userId": "00428686483222286344"
     },
     "user_tz": 420
    },
    "id": "tLcrNQckYNPd",
    "outputId": "ec0b2d69-c4ca-42af-9d50-21877a5e0422"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import pretty_midi\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from scipy.io import wavfile\n",
    "from python_speech_features import mfcc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, TensorDataset\n",
    "#from torcheval.metrics import \n",
    "\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Preproccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1690349299069,
     "user": {
      "displayName": "Jeffrey Thomas",
      "userId": "00428686483222286344"
     },
     "user_tz": 420
    },
    "id": "o5IyPvz4alv4"
   },
   "outputs": [],
   "source": [
    "raw_data_folder = 'Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LpQZNwgAo2LC"
   },
   "outputs": [],
   "source": [
    "def preprocess_midi_into_mel_and_mfcc(midi_file, segment_length=2000, num_cepstral=13):\n",
    "    midi_data = pretty_midi.PrettyMIDI(midi_file)\n",
    "    audio_data = midi_data.synthesize()\n",
    "    wavfile.write(\"temp.wav\", 44100, audio_data.astype(np.float32))\n",
    "\n",
    "    audio, rate = librosa.load(\"temp.wav\")\n",
    "    spectrogram = librosa.feature.melspectrogram(y=audio, sr=rate)\n",
    "    spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "\n",
    "    mfcc_features = mfcc(audio, samplerate=rate, numcep=num_cepstral, winlen=0.025, winstep=0.01, nfft=1103)\n",
    "\n",
    "    # Truncate or pad the spectrogram and MFCC features\n",
    "    min_length = min(spectrogram_db.shape[1], mfcc_features.shape[0])\n",
    "    if min_length % segment_length != 0:\n",
    "        min_length = min_length - (min_length % segment_length)\n",
    "\n",
    "    segments = []\n",
    "    for start in range(0, min_length - segment_length + 1, segment_length):\n",
    "        mel_segment = spectrogram_db[:, start:start + segment_length]\n",
    "        mfcc_segment = mfcc_features[start:start + segment_length, :]\n",
    "        segments.append((mel_segment, mfcc_segment))\n",
    "\n",
    "    os.remove(\"temp.wav\")\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "B8bu9-GBcQDB"
   },
   "outputs": [],
   "source": [
    "def preprocess_data_in_directory(base_dir):\n",
    "    X = []\n",
    "    y = []\n",
    "    X_mel = []\n",
    "    X_mfcc = []\n",
    "\n",
    "    # to show progression\n",
    "    total_files = sum([len(files) for r, d, files in os.walk(base_dir)])\n",
    "    processed_files = 0\n",
    "\n",
    "    composers = os.listdir(base_dir)\n",
    "    for composer in composers:\n",
    "        composer_dir = os.path.join(base_dir, composer)\n",
    "        if os.path.isdir(composer_dir):\n",
    "            for file in os.listdir(composer_dir):\n",
    "                if file.endswith('.mid'):\n",
    "                    file_path = os.path.join(composer_dir, file)\n",
    "                    try:\n",
    "                        segments = preprocess_midi_into_mel_and_mfcc(file_path)\n",
    "\n",
    "                        for mel_segment, mfcc_segment in segments:\n",
    "                            X_mel.append(mel_segment)\n",
    "                            X_mfcc.append(mfcc_segment)\n",
    "                            y.append(composer)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {file_path}: {e}\")\n",
    "                    processed_files += 1\n",
    "                    if processed_files % (total_files // 10) == 0:\n",
    "                        print(f\"Processed {processed_files / total_files * 100}% of files\")\n",
    "    return np.array(X_mel), np.array(X_mfcc), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1202296,
     "status": "ok",
     "timestamp": 1689968687934,
     "user": {
      "displayName": "Jeffrey Thomas",
      "userId": "00428686483222286344"
     },
     "user_tz": 420
    },
    "id": "nKU-Hg2bccyZ",
    "outputId": "767d974d-74c7-420e-cb21-596b2ec81524"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 9.788359788359788% of files\n",
      "Processed 19.576719576719576% of files\n",
      "Processed 29.365079365079367% of files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\lib\\site-packages\\pretty_midi\\pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 39.15343915343915% of files\n",
      "Processed 48.94179894179894% of files\n",
      "Processed 58.730158730158735% of files\n",
      "Processed 68.51851851851852% of files\n",
      "Processed 78.3068783068783% of files\n",
      "Processed 88.09523809523809% of files\n",
      "Processed 9.090909090909092% of files\n",
      "Processed 18.181818181818183% of files\n",
      "Processed 27.27272727272727% of files\n",
      "Processed 36.36363636363637% of files\n",
      "Processed 45.45454545454545% of files\n",
      "Processed 54.54545454545454% of files\n",
      "Processed 63.63636363636363% of files\n",
      "Processed 72.72727272727273% of files\n",
      "Processed 9.090909090909092% of files\n",
      "Processed 18.181818181818183% of files\n",
      "Processed 27.27272727272727% of files\n",
      "Processed 36.36363636363637% of files\n",
      "Processed 45.45454545454545% of files\n",
      "Processed 54.54545454545454% of files\n",
      "Processed 63.63636363636363% of files\n",
      "Processed 72.72727272727273% of files\n"
     ]
    }
   ],
   "source": [
    "X_train_mel, X_train_mfcc, y_train = preprocess_data_in_directory(raw_data_folder + 'train')\n",
    "X_test_mel, X_test_mfcc, y_test = preprocess_data_in_directory(raw_data_folder + 'test')\n",
    "X_dev_mel, X_dev_mfcc, y_dev = preprocess_data_in_directory(raw_data_folder + 'dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Composer Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "y_dev = le.transform(y_dev)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Standardizing Input Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_mel = X_train_mel.mean()\n",
    "std_mel = X_train_mel.std()\n",
    "\n",
    "mean_mfcc = X_train_mfcc.mean()\n",
    "std_mfcc = X_train_mfcc.std() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mel = (X_train_mel - mean_mel) / std_mel\n",
    "X_train_mfcc = (X_train_mfcc - mean_mfcc) / std_mfcc\n",
    "\n",
    "X_dev_mel = (X_dev_mel - mean_mel) / std_mel\n",
    "X_dev_mfcc = (X_dev_mfcc - mean_mfcc) / std_mfcc\n",
    "\n",
    "X_test_mel = (X_test_mel - mean_mel) / std_mel\n",
    "X_test_mfcc = (X_test_mfcc - mean_mfcc) / std_mfcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy to Tensor\n",
    "X_train_mel = torch.from_numpy(X_train_mel).float()\n",
    "X_train_mfcc = torch.from_numpy(X_train_mfcc).float()\n",
    "\n",
    "X_train_mel = X_train_mel.unsqueeze(1)\n",
    "X_train_mfcc = X_train_mfcc.unsqueeze(1)\n",
    "\n",
    "y_train = torch.from_numpy(y_train).float()\n",
    "\n",
    "X_test_mel = torch.from_numpy(X_test_mel).float()\n",
    "X_test_mfcc = torch.from_numpy(X_test_mfcc).float()\n",
    "\n",
    "X_test_mel = X_test_mel.unsqueeze(1)\n",
    "X_test_mfcc = X_test_mfcc.unsqueeze(1)\n",
    "\n",
    "y_test = torch.from_numpy(y_test).float()\n",
    "\n",
    "X_dev_mel = torch.from_numpy(X_dev_mel).float()\n",
    "X_dev_mfcc = torch.from_numpy(X_dev_mfcc).float()\n",
    "\n",
    "X_dev_mel = X_dev_mel.unsqueeze(1)\n",
    "X_dev_mfcc = X_dev_mfcc.unsqueeze(1)\n",
    "\n",
    "y_dev = torch.from_numpy(y_dev).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2908,
     "status": "ok",
     "timestamp": 1690236595709,
     "user": {
      "displayName": "Jeffrey Thomas",
      "userId": "00428686483222286344"
     },
     "user_tz": 420
    },
    "id": "XMmpWmiDEfM-"
   },
   "outputs": [],
   "source": [
    "# Saving Results\n",
    "torch.save(X_train_mel, raw_data_folder + 'X_train_mel.pt')\n",
    "torch.save(X_train_mfcc, raw_data_folder + 'X_train_mfcc.pt')\n",
    "torch.save(y_train, raw_data_folder + 'y_train.pt')\n",
    "torch.save(X_test_mel, raw_data_folder + 'X_test_mel.pt')\n",
    "torch.save(X_test_mfcc, raw_data_folder + 'X_test_mfcc.pt')\n",
    "torch.save(y_test, raw_data_folder + 'y_test.pt')\n",
    "torch.save(X_dev_mel, raw_data_folder + 'X_dev_mel.pt')\n",
    "torch.save(X_dev_mfcc, raw_data_folder + 'X_dev_mfcc.pt')\n",
    "torch.save(y_dev, raw_data_folder + 'y_dev.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the preproccessing section the MFCC and Mel data was extracted from audio files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder = 'Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 2122,
     "status": "ok",
     "timestamp": 1690349322619,
     "user": {
      "displayName": "Jeffrey Thomas",
      "userId": "00428686483222286344"
     },
     "user_tz": 420
    },
    "id": "rsb3sZoIFSCw"
   },
   "outputs": [],
   "source": [
    "X_train_mel = torch.load(raw_data_folder + 'X_train_mel.pt')\n",
    "X_train_mfcc = torch.load(raw_data_folder + 'X_train_mfcc.pt')\n",
    "y_train = torch.load(raw_data_folder + 'y_train.pt')\n",
    "\n",
    "X_test_mel = torch.load(raw_data_folder + 'X_test_mel.pt')\n",
    "X_test_mfcc = torch.load(raw_data_folder + 'X_test_mfcc.pt')\n",
    "y_test = torch.load(raw_data_folder + 'y_test.pt')\n",
    "\n",
    "X_dev_mel = torch.load(raw_data_folder + 'X_dev_mel.pt')\n",
    "X_dev_mfcc = torch.load(raw_data_folder + 'X_dev_mfcc.pt')\n",
    "y_dev = torch.load(raw_data_folder + 'y_dev.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "#X_train_mel = X_train_mel.to(device)\n",
    "#X_train_mfcc = X_train_mfcc.to(device)\n",
    "y_train = y_train.to(device, dtype=torch.int64)\n",
    "#X_test_mel = X_test_mel.to(device)\n",
    "#X_test_mfcc = X_test_mfcc.to(device)\n",
    "y_test = y_test.to(device, dtype=torch.int64)\n",
    "#X_dev_mel = X_dev_mel.to(device)\n",
    "#X_dev_mfcc = X_dev_mfcc.to(device)\n",
    "y_dev = y_dev.to(device, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Getting the Class List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = []\n",
    "composers = os.listdir(raw_data_folder + 'train')\n",
    "for composer in composers:\n",
    "    class_list.append(composer)\n",
    "class_list = class_list[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bach',\n",
       " 'bartok',\n",
       " 'byrd',\n",
       " 'chopin',\n",
       " 'handel',\n",
       " 'hummel',\n",
       " 'mendelssohn',\n",
       " 'mozart']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Weighted Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1690349325584,
     "user": {
      "displayName": "Jeffrey Thomas",
      "userId": "00428686483222286344"
     },
     "user_tz": 420
    },
    "id": "1XpHl97poGMg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 234\n",
      "1: 111\n",
      "2: 194\n",
      "3: 221\n",
      "4: 197\n",
      "5: 500\n",
      "6: 227\n",
      "7: 425\n",
      "8: 265\n"
     ]
    }
   ],
   "source": [
    "# Get unique values and their counts\n",
    "unique_values, counts = torch.unique(y_train.cpu(), return_counts=True)\n",
    "\n",
    "# Print unique values and their counts\n",
    "for value, count in zip(unique_values, counts):\n",
    "    print(f'{value}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2374"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting Class Weights\n",
    "target = torch.Tensor.numpy(y_train.cpu())\n",
    "target = target.astype('int')\n",
    "class_sample_counts = np.array([len(np.where(target == t)[0]) for t in np.unique(target)])\n",
    "class_weights = [1/class_sample_counts[i] for i in target]\n",
    "len(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = WeightedRandomSampler(weights = class_weights, num_samples = len(class_weights), replacement = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Mel DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelDataset(Dataset):\n",
    "    def __init__(self, X_mel, y, class_list):\n",
    "        self.X_mel = X_mel\n",
    "        self.y = y\n",
    "        self.class_list = class_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_mel)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_mel[idx], self.y[idx]\n",
    "\n",
    "\n",
    "    def classes(self):\n",
    "        # Return the list of class labels\n",
    "        return self.class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_mel = MelDataset(X_train_mel, y_train, class_list)\n",
    "test_dataset_mel = MelDataset(X_test_mel, y_test, class_list)\n",
    "val_dataset_mel = MelDataset(X_dev_mel, y_dev, class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Weighted Sampling\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader\n",
    "train_loader_mel = DataLoader(train_dataset_mel, batch_size=batch_size, shuffle=True)\n",
    "test_loader_mel = DataLoader(test_dataset_mel, batch_size=batch_size, shuffle=False)\n",
    "val_loader_mel = DataLoader(val_dataset_mel, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Weighted Sampling\n",
    "weighted_train_loader_mel = DataLoader(train_dataset_mel, sampler = sampler,  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Making MFCC Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCCDataset(Dataset):\n",
    "    def __init__(self, X_mfcc, y, class_list):\n",
    "        self.X_mfcc = X_mfcc\n",
    "        self.y = y\n",
    "        self.class_list = class_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_mfcc)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_mfcc[idx], self.y[idx]\n",
    "\n",
    "\n",
    "    def classes(self):\n",
    "        # Return the list of class labels\n",
    "        return self.class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mfcc = X_train_mfcc.squeeze(1)\n",
    "X_test_mfcc = X_test_mfcc.squeeze(1)\n",
    "X_dev_mfcc = X_dev_mfcc.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_mfcc = MFCCDataset(X_train_mfcc, y_train, class_list) \n",
    "test_dataset_mfcc = MFCCDataset(X_test_mfcc, y_test, class_list) \n",
    "val_dataset_mfcc = MFCCDataset(X_dev_mfcc, y_dev, class_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Weighted Sampling\n",
    "train_loader_mfcc = DataLoader(train_dataset_mfcc, batch_size=batch_size, shuffle=True)\n",
    "test_loader_mfcc = DataLoader(test_dataset_mfcc, batch_size=batch_size, shuffle=False)\n",
    "val_loader_mfcc = DataLoader(val_dataset_mfcc, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Weighted Sampling\n",
    "weighted_train_loader_mfcc = DataLoader(train_dataset_mfcc, sampler = sampler,  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making MEL and MFCC Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, X_mel, X_mfcc, y, class_list):\n",
    "        self.X_mel = X_mel\n",
    "        self.X_mfcc = X_mfcc\n",
    "        self.y = y\n",
    "        self.class_list = class_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_mfcc)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_mel[idx], self.X_mfcc[idx], self.y[idx]\n",
    "\n",
    "    def classes(self):\n",
    "        # Return the list of class labels\n",
    "        return self.class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_combined = CombinedDataset(X_train_mel, X_train_mfcc, y_train, class_list)\n",
    "test_dataset_combined = CombinedDataset(X_test_mel, X_test_mfcc, y_test, class_list)\n",
    "val_dataset_combined = CombinedDataset(X_dev_mel, X_dev_mfcc, y_dev, class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Weighted Sampling\n",
    "train_loader_combined = DataLoader(train_dataset_combined, batch_size=batch_size, shuffle=True)\n",
    "test_loader_combined = DataLoader(test_dataset_combined, batch_size=batch_size, shuffle=False)\n",
    "val_loader_combined = DataLoader(val_dataset_combined, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Weighted Sampling\n",
    "weighted_train_loader_combined = DataLoader(train_dataset_combined, sampler = sampler,  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MEL_CNN(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, init_filters, num_classes, dropout_rate=0.2):\n",
    "        super(MEL_CNN, self).__init__()\n",
    "        self.hidden_layers = num_hidden_layers\n",
    "        \n",
    "        #Pooling Layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Input Layer\n",
    "        self.layers = nn.ModuleDict()\n",
    "        self.layers[\"input_Conv\"] = nn.Conv2d(1, init_filters, kernel_size=3, padding=1)\n",
    "        self.layers[\"input_batch_norm\"] = nn.BatchNorm2d(init_filters)\n",
    "        \n",
    "        # Hidden Layers\n",
    "        for i in range(num_hidden_layers):\n",
    "            in_channels = 2 ** i * init_filters\n",
    "            out_channels = in_channels * 2\n",
    "            self.layers[f\"hidden_{i}\"] = nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1)\n",
    "            self.layers[f\"batch_norm_{i}\"] = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Last Layer\n",
    "        final_in_channels = 2 ** num_hidden_layers * init_filters\n",
    "        final_out_channels = final_in_channels * 4\n",
    "        self.layers[\"output_Conv\"] = nn.Conv2d(final_in_channels, final_out_channels, kernel_size = 1)\n",
    "        self.layers[\"output_batch_norm\"] = nn.BatchNorm2d(final_out_channels)\n",
    "        self.layers[\"linear\"] = nn.Linear(final_out_channels, 128)\n",
    "        self.layers[\"dropout\"] = nn.Dropout(p = dropout_rate)\n",
    "        self.layers[\"linear_output_1\"] = nn.Linear(128, 64)\n",
    "        self.layers[\"linear_output_2\"] = nn.Linear(64, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.layers[\"input_batch_norm\"](self.layers[\"input_Conv\"](x))))\n",
    "        for i in range(self.hidden_layers):\n",
    "            x = self.pool(nn.functional.relu(self.layers[f\"batch_norm_{i}\"](self.layers[f\"hidden_{i}\"](x))))\n",
    "            \n",
    "        x = self.pool(nn.functional.relu(self.layers[\"output_batch_norm\"](self.layers[\"output_Conv\"](x))))\n",
    "        x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = nn.functional.relu(self.layers[\"linear\"](x))\n",
    "        x = self.layers[\"dropout\"](x)\n",
    "        x = nn.functional.relu(self.layers[\"linear_output_1\"](x))\n",
    "        x = self.layers[\"linear_output_2\"](x)\n",
    "        return nn.functional.softmax(x, dim = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCC_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(MFCC_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(x.device)  # Multiply by 2 because it's bidirectional\n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(x.device)  # Multiply by 2 because it's bidirectional\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # shape = (batch_size, seq_length, hidden_size*2)\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM_Ensemble(nn.Module):\n",
    "    def __init__(self, model_CNN, model_LSTM, num_class):\n",
    "        super(CNN_LSTM_Ensemble, self).__init__()\n",
    "        self.LSTM = model_LSTM\n",
    "        self.CNN = model_CNN\n",
    "        \n",
    "        self.outward = nn.Linear(num_class, num_class)\n",
    "    \n",
    "    def forward(self, MEL, MFCC):\n",
    "        output_CNN = self.CNN(MEL)\n",
    "        output_LSTM = self.LSTM(MFCC)\n",
    "        output = output_CNN + output_LSTM\n",
    "        \n",
    "        output = self.outward(output)\n",
    "        return nn.functional.softmax(output, dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single(model, optimizer, scheduler, train_loader, test_loader, num_epochs=100, patience=10):\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    best_test_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        train_iter = iter(train_loader)\n",
    "        test_iter = iter(test_loader)\n",
    "\n",
    "        for batch_idx, (mel_inputs, labels) in enumerate(train_iter):\n",
    "            mel_inputs, labels = mel_inputs.to(device), labels.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(mel_inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            labels = labels.long()\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * mel_inputs.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        train_accuracy = train_correct / train_total\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0.0\n",
    "            test_correct = 0\n",
    "            test_total = 0\n",
    "\n",
    "            for mel_inputs, labels in test_loader:\n",
    "                mel_inputs, labels = mel_inputs.to(device), labels.to(device)\n",
    "                # Forward pass\n",
    "                labels = labels.long()\n",
    "                outputs = model(mel_inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Compute accuracy and loss\n",
    "                test_loss += criterion(outputs, labels).item() * mel_inputs.size(0)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            test_loss /= len(test_loader.dataset)\n",
    "            test_losses.append(test_loss)\n",
    "\n",
    "            test_accuracy = test_correct / test_total\n",
    "            test_accuracies.append(test_accuracy)\n",
    "\n",
    "        accuracy = 100 * test_correct / test_total\n",
    "        print(f'Epoch {epoch}, train loss: {train_loss:.4f}, train accuracy: {train_accuracy:.2f}, test loss: {test_loss:.4f}, test accuracy: {test_accuracy:.2f}, learning rate: {scheduler.get_lr()[0]}')\n",
    "\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            epochs_no_improve = 0\n",
    "            print('Saving model!')\n",
    "            torch.save(model.state_dict(), 'cnn_lsmt_model.pt')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve in [20, 30, 40]:\n",
    "                print('Stepping lr_scheduler')\n",
    "                scheduler.step()\n",
    "            if epochs_no_improve == patience:\n",
    "                print('Early stopping!')\n",
    "                break\n",
    "\n",
    "    return train_losses, test_losses, train_accuracies, test_accuracies, best_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(model, optimizer, scheduler, train_loader, test_loader, num_epochs=100, patience=10, model_id = \"Best_Model.pt\"):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    best_test_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for batch_idx, (mel_inputs, mfcc_inputs, labels) in enumerate(train_loader):\n",
    "            mel_inputs, mfcc_inputs, labels = mel_inputs.to(device), mfcc_inputs.to(device), labels.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(mel_inputs, mfcc_inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            labels = labels.long()\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * mel_inputs.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        train_accuracy = train_correct / train_total\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0.0\n",
    "            test_correct = 0\n",
    "            test_total = 0\n",
    "\n",
    "            for mel_inputs, mfcc_inputs, labels in test_loader:\n",
    "                mel_inputs, mfcc_inputs, labels = mel_inputs.to(device), mfcc_inputs.to(device), labels.to(device)\n",
    "                # Forward pass\n",
    "                outputs = model(mel_inputs, mfcc_inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Compute accuracy and loss\n",
    "                test_loss += criterion(outputs, labels).item() * mel_inputs.size(0)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            test_loss /= len(test_loader.dataset)\n",
    "            test_losses.append(test_loss)\n",
    "\n",
    "            test_accuracy = test_correct / test_total\n",
    "            test_accuracies.append(test_accuracy)\n",
    "\n",
    "        accuracy = 100 * test_correct / test_total\n",
    "        print(f'Epoch {epoch}, train loss: {train_loss:.4f}, train accuracy: {train_accuracy:.2f}, test loss: {test_loss:.4f}, test accuracy: {test_accuracy:.2f}, learning rate: {scheduler.get_lr()[0]}')\n",
    "\n",
    "        # if epoch > 5 and train_loss > 2.1:\n",
    "        #   print('Too little progress being made')\n",
    "        #   break\n",
    "\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            epochs_no_improve = 0\n",
    "            print('Saving model!')\n",
    "            torch.save(model.state_dict(), model_id)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve in [20, 30, 40]:\n",
    "                print('Stepping lr_scheduler')\n",
    "                scheduler.step()\n",
    "            if epochs_no_improve == patience:\n",
    "                print('Early stopping!')\n",
    "                break\n",
    "\n",
    "    return train_losses, test_losses, train_accuracies, test_accuracies, best_test_loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Test CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (layers): ModuleDict(\n",
      "    (input_Conv): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (input_batch_norm): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (hidden_0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batch_norm_0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (hidden_1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (output_Conv): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (output_batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (linear_output_1): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (linear_output_2): Linear(in_features=64, out_features=9, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "hl = 2\n",
    "init = 8\n",
    "num_classes = len(torch.unique(y_train.cpu()))\n",
    "dropout = 0.5\n",
    "test_model = MEL_CNN(hl, init, num_classes, dropout)\n",
    "\n",
    "print(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 10\n",
    "num_epochs = 50\n",
    "lr = 0.001\n",
    "weight_decay=0.005\n",
    "optimizer = torch.optim.Adam(test_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "test_model, optimizer, weighted_train_loader_mel, test_loader_mel = accelerator.prepare(\n",
    "test_model, optimizer, weighted_train_loader_mel, test_loader_mel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 1.9713, train accuracy: 0.40, test loss: 2.0165, test accuracy: 0.38, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 1, train loss: 1.9622, train accuracy: 0.41, test loss: 2.0151, test accuracy: 0.34, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 2, train loss: 2.0005, train accuracy: 0.36, test loss: 2.0101, test accuracy: 0.35, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 3, train loss: 1.9742, train accuracy: 0.40, test loss: 1.9875, test accuracy: 0.37, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 4, train loss: 1.9774, train accuracy: 0.40, test loss: 1.9772, test accuracy: 0.40, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 5, train loss: 1.9762, train accuracy: 0.40, test loss: 1.9916, test accuracy: 0.38, learning rate: 0.001\n",
      "Epoch 6, train loss: 1.9750, train accuracy: 0.40, test loss: 1.9766, test accuracy: 0.40, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 7, train loss: 1.9761, train accuracy: 0.40, test loss: 1.9733, test accuracy: 0.41, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 8, train loss: 1.9836, train accuracy: 0.39, test loss: 1.9835, test accuracy: 0.40, learning rate: 0.001\n",
      "Epoch 9, train loss: 1.9752, train accuracy: 0.40, test loss: 2.1336, test accuracy: 0.18, learning rate: 0.001\n",
      "Epoch 10, train loss: 1.9697, train accuracy: 0.41, test loss: 1.9695, test accuracy: 0.44, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 11, train loss: 1.9670, train accuracy: 0.41, test loss: 2.0757, test accuracy: 0.32, learning rate: 0.001\n",
      "Epoch 12, train loss: 1.9605, train accuracy: 0.42, test loss: 2.0627, test accuracy: 0.27, learning rate: 0.001\n",
      "Epoch 13, train loss: 1.9639, train accuracy: 0.41, test loss: 1.9993, test accuracy: 0.35, learning rate: 0.001\n",
      "Epoch 14, train loss: 1.9659, train accuracy: 0.40, test loss: 1.9719, test accuracy: 0.39, learning rate: 0.001\n",
      "Epoch 15, train loss: 1.9512, train accuracy: 0.43, test loss: 1.9752, test accuracy: 0.39, learning rate: 0.001\n",
      "Epoch 16, train loss: 1.9625, train accuracy: 0.41, test loss: 2.0049, test accuracy: 0.36, learning rate: 0.001\n",
      "Epoch 17, train loss: 1.9614, train accuracy: 0.42, test loss: 2.0183, test accuracy: 0.35, learning rate: 0.001\n",
      "Epoch 18, train loss: 1.9633, train accuracy: 0.41, test loss: 2.0179, test accuracy: 0.36, learning rate: 0.001\n",
      "Epoch 19, train loss: 1.9582, train accuracy: 0.43, test loss: 2.0728, test accuracy: 0.27, learning rate: 0.001\n",
      "Epoch 20, train loss: 1.9670, train accuracy: 0.41, test loss: 1.9920, test accuracy: 0.40, learning rate: 0.001\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "train_losses, test_losses, train_acc, test_acc, best_loss = train_single(test_model, optimizer, scheduler, weighted_train_loader_mel, test_loader_mel, num_epochs, patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Test LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCC_LSTM(\n",
      "  (lstm): LSTM(13, 32, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=64, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 13\n",
    "hidden_size = 32\n",
    "num_layers = 2\n",
    "num_classes = len(torch.unique(y_train.cpu()))\n",
    "\n",
    "lstm_test_model = MFCC_LSTM(input_size, hidden_size, num_layers, num_classes)\n",
    "\n",
    "print(lstm_test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 10\n",
    "num_epochs = 50\n",
    "lr = 0.001\n",
    "weight_decay = 0.01\n",
    "\n",
    "optimizer = torch.optim.Adam(lstm_test_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "lstm_test_model, optimizer, weighted_train_loader_mfcc, test_loader_mfcc = accelerator.prepare(\n",
    "lstm_test_model, optimizer, weighted_train_loader_mfcc, test_loader_mfcc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 2.1945, train accuracy: 0.14, test loss: 2.1974, test accuracy: 0.11, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 1, train loss: 2.1787, train accuracy: 0.17, test loss: 2.1882, test accuracy: 0.16, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 2, train loss: 2.1482, train accuracy: 0.17, test loss: 2.1825, test accuracy: 0.13, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 3, train loss: 2.1066, train accuracy: 0.18, test loss: 2.1300, test accuracy: 0.15, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 4, train loss: 2.1332, train accuracy: 0.16, test loss: 2.1464, test accuracy: 0.10, learning rate: 0.001\n",
      "Epoch 5, train loss: 2.0835, train accuracy: 0.19, test loss: 2.1280, test accuracy: 0.10, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 6, train loss: 2.0502, train accuracy: 0.20, test loss: 2.1192, test accuracy: 0.13, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 7, train loss: 2.1069, train accuracy: 0.17, test loss: 2.1305, test accuracy: 0.14, learning rate: 0.001\n",
      "Epoch 8, train loss: 2.0446, train accuracy: 0.21, test loss: 2.1229, test accuracy: 0.12, learning rate: 0.001\n",
      "Epoch 9, train loss: 2.0389, train accuracy: 0.21, test loss: 2.1471, test accuracy: 0.08, learning rate: 0.001\n",
      "Epoch 10, train loss: 2.1089, train accuracy: 0.17, test loss: 2.1478, test accuracy: 0.15, learning rate: 0.001\n",
      "Epoch 11, train loss: 2.0407, train accuracy: 0.19, test loss: 1.9792, test accuracy: 0.18, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 12, train loss: 1.9878, train accuracy: 0.19, test loss: 1.9382, test accuracy: 0.17, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 13, train loss: 1.9396, train accuracy: 0.21, test loss: 2.1063, test accuracy: 0.16, learning rate: 0.001\n",
      "Epoch 14, train loss: 2.1065, train accuracy: 0.16, test loss: 2.1168, test accuracy: 0.11, learning rate: 0.001\n",
      "Epoch 15, train loss: 2.0401, train accuracy: 0.19, test loss: 2.0872, test accuracy: 0.11, learning rate: 0.001\n",
      "Epoch 16, train loss: 1.9818, train accuracy: 0.19, test loss: 2.0662, test accuracy: 0.11, learning rate: 0.001\n",
      "Epoch 17, train loss: 2.0847, train accuracy: 0.20, test loss: 2.0351, test accuracy: 0.13, learning rate: 0.001\n",
      "Epoch 18, train loss: 1.9933, train accuracy: 0.22, test loss: 1.9916, test accuracy: 0.15, learning rate: 0.001\n",
      "Epoch 19, train loss: 1.9690, train accuracy: 0.23, test loss: 1.9850, test accuracy: 0.16, learning rate: 0.001\n",
      "Epoch 20, train loss: 1.9838, train accuracy: 0.22, test loss: 2.0107, test accuracy: 0.16, learning rate: 0.001\n",
      "Epoch 21, train loss: 1.9691, train accuracy: 0.23, test loss: 1.9560, test accuracy: 0.13, learning rate: 0.001\n",
      "Epoch 22, train loss: 1.9555, train accuracy: 0.22, test loss: 1.9823, test accuracy: 0.13, learning rate: 0.001\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "train_losses, test_losses, train_acc, test_acc, best_loss = train_single(lstm_test_model, optimizer, scheduler, weighted_train_loader_mfcc, test_loader_mfcc, num_epochs, patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Testing Ensemble Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEL_CNN(\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (layers): ModuleDict(\n",
      "    (input_Conv): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (input_batch_norm): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (hidden_0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batch_norm_0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (hidden_1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (output_Conv): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (output_batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (linear_output_1): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (linear_output_2): Linear(in_features=64, out_features=9, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Mel Model\n",
    "hl = 2\n",
    "init = 8\n",
    "num_classes = len(torch.unique(y_train.cpu()))\n",
    "dropout = 0.5\n",
    "mel_model = MEL_CNN(hl, init, num_classes, dropout)\n",
    "\n",
    "print(mel_model)\n",
    "\n",
    "mel_model = mel_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCC_LSTM(\n",
      "  (lstm): LSTM(13, 32, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=64, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# MFCC model\n",
    "input_size = 13\n",
    "hidden_size = 32\n",
    "num_layers = 2\n",
    "num_classes = len(torch.unique(y_train.cpu()))\n",
    "\n",
    "mfcc_model = MFCC_LSTM(input_size, hidden_size, num_layers, num_classes)\n",
    "\n",
    "print(mfcc_model)\n",
    "mfcc_model = mfcc_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_LSTM_Ensemble(\n",
      "  (LSTM): MFCC_LSTM(\n",
      "    (lstm): LSTM(13, 32, num_layers=2, batch_first=True, bidirectional=True)\n",
      "    (fc): Linear(in_features=64, out_features=9, bias=True)\n",
      "  )\n",
      "  (CNN): MEL_CNN(\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (layers): ModuleDict(\n",
      "      (input_Conv): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (input_batch_norm): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (hidden_0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (batch_norm_0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (hidden_1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (output_Conv): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (output_batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "      (linear_output_1): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (linear_output_2): Linear(in_features=64, out_features=9, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (outward): Linear(in_features=9, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Ensemble\n",
    "test_ensemble =  CNN_LSTM_Ensemble(mel_model, mfcc_model, num_classes)\n",
    "print(test_ensemble)\n",
    "test_ensemble = test_ensemble.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 10\n",
    "num_epochs = 50\n",
    "lr = 0.001\n",
    "weight_decay = 0.01\n",
    "\n",
    "optimizer = torch.optim.Adam(test_ensemble.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "test_ensemble, optimizer, weighted_train_loader_combined, test_loader_combined = accelerator.prepare(\n",
    "test_ensemble, optimizer, weighted_train_loader_combined, test_loader_combined\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 2.1979, train accuracy: 0.10, test loss: 2.1982, test accuracy: 0.05, learning rate: 0.001\n",
      "Saving model!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[122], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_losses, test_losses, train_acc, test_acc, best_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ensemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_ensemble\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweighted_train_loader_combined\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader_combined\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[104], line 81\u001b[0m, in \u001b[0;36mtrain_ensemble\u001b[1;34m(model, optimizer, scheduler, train_loader, test_loader, num_epochs, patience)\u001b[0m\n\u001b[0;32m     79\u001b[0m     epochs_no_improve \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSaving model!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 81\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[43mmodel_id\u001b[49m)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     83\u001b[0m     epochs_no_improve \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_id' is not defined"
     ]
    }
   ],
   "source": [
    "train_losses, test_losses, train_acc, test_acc, best_loss = train_ensemble(test_ensemble, optimizer, scheduler, weighted_train_loader_combined, test_loader_combined, num_epochs, patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Training CNN on MEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1690255927716,
     "user": {
      "displayName": "Jeffrey Thomas",
      "userId": "00428686483222286344"
     },
     "user_tz": 420
    },
    "id": "bwLI-Wwo1u7e"
   },
   "outputs": [],
   "source": [
    "patience = 60\n",
    "num_epochs = 200\n",
    "lr = 0.0001\n",
    "dropout_rate = 0.5\n",
    "weight_decay=0.005\n",
    "num_classes = len(torch.unique(y_train))\n",
    "# Initialize the model\n",
    "mel_model = Mel_CNN(num_classes, dropout_rate)\n",
    "mel_model = mel_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1690255928583,
     "user": {
      "displayName": "Jeffrey Thomas",
      "userId": "00428686483222286344"
     },
     "user_tz": 420
    },
    "id": "gzfv-OxX14bk"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(mel_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1690255929333,
     "user": {
      "displayName": "Jeffrey Thomas",
      "userId": "00428686483222286344"
     },
     "user_tz": 420
    },
    "id": "dbGnBwwp17G9"
   },
   "outputs": [],
   "source": [
    "accelerator = Accelerator()\n",
    "mel_model, optimizer, train_loader_mel, test_loader_mel = accelerator.prepare(\n",
    "mel_model, optimizer, train_loader_mel, test_loader_mel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1247545,
     "status": "ok",
     "timestamp": 1690257178233,
     "user": {
      "displayName": "Jeffrey Thomas",
      "userId": "00428686483222286344"
     },
     "user_tz": 420
    },
    "id": "fpb7aEkU2BoF",
    "outputId": "c4d374cc-d0a6-4c7d-8cb1-3241ffecd559"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 2.1830, train accuracy: 0.16, test loss: 2.1567, test accuracy: 0.20, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 1, train loss: 2.1369, train accuracy: 0.24, test loss: 2.0900, test accuracy: 0.36, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 2, train loss: 2.0782, train accuracy: 0.30, test loss: 2.0360, test accuracy: 0.39, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 3, train loss: 2.0460, train accuracy: 0.32, test loss: 2.0097, test accuracy: 0.38, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 4, train loss: 2.0115, train accuracy: 0.37, test loss: 2.0636, test accuracy: 0.28, learning rate: 0.0001\n",
      "Epoch 5, train loss: 1.9847, train accuracy: 0.39, test loss: 2.0048, test accuracy: 0.37, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 6, train loss: 1.9604, train accuracy: 0.42, test loss: 1.9548, test accuracy: 0.43, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 7, train loss: 1.9310, train accuracy: 0.47, test loss: 1.9188, test accuracy: 0.49, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 8, train loss: 1.9122, train accuracy: 0.48, test loss: 2.0333, test accuracy: 0.36, learning rate: 0.0001\n",
      "Epoch 9, train loss: 1.8954, train accuracy: 0.49, test loss: 1.8953, test accuracy: 0.50, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 10, train loss: 1.8849, train accuracy: 0.49, test loss: 2.0016, test accuracy: 0.38, learning rate: 0.0001\n",
      "Epoch 11, train loss: 1.8800, train accuracy: 0.50, test loss: 1.9340, test accuracy: 0.43, learning rate: 0.0001\n",
      "Epoch 12, train loss: 1.8813, train accuracy: 0.50, test loss: 1.9735, test accuracy: 0.38, learning rate: 0.0001\n",
      "Epoch 13, train loss: 1.8712, train accuracy: 0.50, test loss: 1.8652, test accuracy: 0.53, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 14, train loss: 1.8645, train accuracy: 0.51, test loss: 1.9668, test accuracy: 0.41, learning rate: 0.0001\n",
      "Epoch 15, train loss: 1.8634, train accuracy: 0.52, test loss: 1.9048, test accuracy: 0.46, learning rate: 0.0001\n",
      "Epoch 16, train loss: 1.8718, train accuracy: 0.51, test loss: 1.8872, test accuracy: 0.50, learning rate: 0.0001\n",
      "Epoch 17, train loss: 1.8582, train accuracy: 0.52, test loss: 2.0591, test accuracy: 0.29, learning rate: 0.0001\n",
      "Epoch 18, train loss: 1.8565, train accuracy: 0.52, test loss: 1.8962, test accuracy: 0.47, learning rate: 0.0001\n",
      "Epoch 19, train loss: 1.8503, train accuracy: 0.52, test loss: 1.9234, test accuracy: 0.45, learning rate: 0.0001\n",
      "Epoch 20, train loss: 1.8573, train accuracy: 0.52, test loss: 1.8982, test accuracy: 0.47, learning rate: 0.0001\n",
      "Epoch 21, train loss: 1.8489, train accuracy: 0.52, test loss: 1.9249, test accuracy: 0.44, learning rate: 0.0001\n",
      "Epoch 22, train loss: 1.8364, train accuracy: 0.54, test loss: 1.8830, test accuracy: 0.49, learning rate: 0.0001\n",
      "Epoch 23, train loss: 1.8382, train accuracy: 0.53, test loss: 2.0228, test accuracy: 0.34, learning rate: 0.0001\n",
      "Epoch 24, train loss: 1.8433, train accuracy: 0.53, test loss: 1.8772, test accuracy: 0.49, learning rate: 0.0001\n",
      "Epoch 25, train loss: 1.8311, train accuracy: 0.54, test loss: 1.9998, test accuracy: 0.36, learning rate: 0.0001\n",
      "Epoch 26, train loss: 1.8260, train accuracy: 0.54, test loss: 1.8930, test accuracy: 0.49, learning rate: 0.0001\n",
      "Epoch 27, train loss: 1.8395, train accuracy: 0.53, test loss: 1.9317, test accuracy: 0.44, learning rate: 0.0001\n",
      "Epoch 28, train loss: 1.8307, train accuracy: 0.53, test loss: 1.9131, test accuracy: 0.46, learning rate: 0.0001\n",
      "Epoch 29, train loss: 1.8301, train accuracy: 0.54, test loss: 1.8556, test accuracy: 0.51, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 30, train loss: 1.8196, train accuracy: 0.54, test loss: 1.9286, test accuracy: 0.44, learning rate: 0.0001\n",
      "Epoch 31, train loss: 1.8331, train accuracy: 0.54, test loss: 1.9810, test accuracy: 0.40, learning rate: 0.0001\n",
      "Epoch 32, train loss: 1.8260, train accuracy: 0.54, test loss: 1.9151, test accuracy: 0.48, learning rate: 0.0001\n",
      "Epoch 33, train loss: 1.8189, train accuracy: 0.54, test loss: 1.8856, test accuracy: 0.49, learning rate: 0.0001\n",
      "Epoch 34, train loss: 1.8065, train accuracy: 0.55, test loss: 1.8557, test accuracy: 0.49, learning rate: 0.0001\n",
      "Epoch 35, train loss: 1.8248, train accuracy: 0.54, test loss: 1.9023, test accuracy: 0.47, learning rate: 0.0001\n",
      "Epoch 36, train loss: 1.8154, train accuracy: 0.54, test loss: 1.9753, test accuracy: 0.37, learning rate: 0.0001\n",
      "Epoch 37, train loss: 1.8042, train accuracy: 0.55, test loss: 1.9228, test accuracy: 0.44, learning rate: 0.0001\n",
      "Epoch 38, train loss: 1.8163, train accuracy: 0.54, test loss: 2.0322, test accuracy: 0.33, learning rate: 0.0001\n",
      "Epoch 39, train loss: 1.8005, train accuracy: 0.56, test loss: 1.8369, test accuracy: 0.52, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 40, train loss: 1.7964, train accuracy: 0.57, test loss: 1.8490, test accuracy: 0.53, learning rate: 0.0001\n",
      "Epoch 41, train loss: 1.7929, train accuracy: 0.58, test loss: 2.0649, test accuracy: 0.29, learning rate: 0.0001\n",
      "Epoch 42, train loss: 1.7921, train accuracy: 0.59, test loss: 1.9623, test accuracy: 0.39, learning rate: 0.0001\n",
      "Epoch 43, train loss: 1.7841, train accuracy: 0.60, test loss: 2.0605, test accuracy: 0.27, learning rate: 0.0001\n",
      "Epoch 44, train loss: 1.7715, train accuracy: 0.61, test loss: 2.0154, test accuracy: 0.32, learning rate: 0.0001\n",
      "Epoch 45, train loss: 1.7568, train accuracy: 0.63, test loss: 2.0108, test accuracy: 0.35, learning rate: 0.0001\n",
      "Epoch 46, train loss: 1.7784, train accuracy: 0.60, test loss: 1.8534, test accuracy: 0.51, learning rate: 0.0001\n",
      "Epoch 47, train loss: 1.7584, train accuracy: 0.62, test loss: 1.9032, test accuracy: 0.46, learning rate: 0.0001\n",
      "Epoch 48, train loss: 1.7489, train accuracy: 0.63, test loss: 1.8798, test accuracy: 0.49, learning rate: 0.0001\n",
      "Epoch 49, train loss: 1.7451, train accuracy: 0.63, test loss: 1.9135, test accuracy: 0.48, learning rate: 0.0001\n",
      "Epoch 50, train loss: 1.7411, train accuracy: 0.63, test loss: 1.9368, test accuracy: 0.43, learning rate: 0.0001\n",
      "Epoch 51, train loss: 1.7526, train accuracy: 0.63, test loss: 1.8936, test accuracy: 0.49, learning rate: 0.0001\n",
      "Epoch 52, train loss: 1.7377, train accuracy: 0.65, test loss: 1.8348, test accuracy: 0.52, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 53, train loss: 1.7290, train accuracy: 0.65, test loss: 1.9172, test accuracy: 0.42, learning rate: 0.0001\n",
      "Epoch 54, train loss: 1.7254, train accuracy: 0.66, test loss: 1.9397, test accuracy: 0.41, learning rate: 0.0001\n",
      "Epoch 55, train loss: 1.7296, train accuracy: 0.66, test loss: 1.8892, test accuracy: 0.49, learning rate: 0.0001\n",
      "Epoch 56, train loss: 1.7334, train accuracy: 0.65, test loss: 1.8463, test accuracy: 0.54, learning rate: 0.0001\n",
      "Epoch 57, train loss: 1.6998, train accuracy: 0.69, test loss: 1.8134, test accuracy: 0.56, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 58, train loss: 1.7041, train accuracy: 0.70, test loss: 1.7913, test accuracy: 0.59, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 59, train loss: 1.6837, train accuracy: 0.72, test loss: 1.8248, test accuracy: 0.55, learning rate: 0.0001\n",
      "Epoch 60, train loss: 1.6971, train accuracy: 0.70, test loss: 2.0115, test accuracy: 0.33, learning rate: 0.0001\n",
      "Epoch 61, train loss: 1.6878, train accuracy: 0.71, test loss: 1.7862, test accuracy: 0.59, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 62, train loss: 1.6674, train accuracy: 0.73, test loss: 2.0165, test accuracy: 0.34, learning rate: 0.0001\n",
      "Epoch 63, train loss: 1.6677, train accuracy: 0.73, test loss: 2.0641, test accuracy: 0.29, learning rate: 0.0001\n",
      "Epoch 64, train loss: 1.6646, train accuracy: 0.73, test loss: 1.7584, test accuracy: 0.63, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 65, train loss: 1.6577, train accuracy: 0.74, test loss: 1.7809, test accuracy: 0.62, learning rate: 0.0001\n",
      "Epoch 66, train loss: 1.6683, train accuracy: 0.73, test loss: 1.8906, test accuracy: 0.48, learning rate: 0.0001\n",
      "Epoch 67, train loss: 1.6550, train accuracy: 0.74, test loss: 2.0391, test accuracy: 0.33, learning rate: 0.0001\n",
      "Epoch 68, train loss: 1.6595, train accuracy: 0.74, test loss: 1.8096, test accuracy: 0.56, learning rate: 0.0001\n",
      "Epoch 69, train loss: 1.6444, train accuracy: 0.75, test loss: 1.9748, test accuracy: 0.39, learning rate: 0.0001\n",
      "Epoch 70, train loss: 1.6397, train accuracy: 0.75, test loss: 1.8899, test accuracy: 0.49, learning rate: 0.0001\n",
      "Epoch 71, train loss: 1.6392, train accuracy: 0.76, test loss: 1.8894, test accuracy: 0.49, learning rate: 0.0001\n",
      "Epoch 72, train loss: 1.6304, train accuracy: 0.77, test loss: 1.8947, test accuracy: 0.46, learning rate: 0.0001\n",
      "Epoch 73, train loss: 1.6240, train accuracy: 0.77, test loss: 2.0616, test accuracy: 0.28, learning rate: 0.0001\n",
      "Epoch 74, train loss: 1.6181, train accuracy: 0.78, test loss: 1.8826, test accuracy: 0.49, learning rate: 0.0001\n",
      "Epoch 75, train loss: 1.6113, train accuracy: 0.79, test loss: 1.9291, test accuracy: 0.44, learning rate: 0.0001\n",
      "Epoch 76, train loss: 1.6102, train accuracy: 0.79, test loss: 2.1079, test accuracy: 0.23, learning rate: 0.0001\n",
      "Epoch 77, train loss: 1.6121, train accuracy: 0.78, test loss: 1.9251, test accuracy: 0.46, learning rate: 0.0001\n",
      "Epoch 78, train loss: 1.6070, train accuracy: 0.79, test loss: 1.8588, test accuracy: 0.52, learning rate: 0.0001\n",
      "Epoch 79, train loss: 1.5982, train accuracy: 0.80, test loss: 1.8276, test accuracy: 0.54, learning rate: 0.0001\n",
      "Epoch 80, train loss: 1.5996, train accuracy: 0.80, test loss: 1.9666, test accuracy: 0.41, learning rate: 0.0001\n",
      "Epoch 81, train loss: 1.6044, train accuracy: 0.79, test loss: 1.7994, test accuracy: 0.57, learning rate: 0.0001\n",
      "Epoch 82, train loss: 1.6067, train accuracy: 0.79, test loss: 1.8794, test accuracy: 0.51, learning rate: 0.0001\n",
      "Epoch 83, train loss: 1.5956, train accuracy: 0.80, test loss: 2.0077, test accuracy: 0.34, learning rate: 0.0001\n",
      "Epoch 84, train loss: 1.5947, train accuracy: 0.80, test loss: 1.8288, test accuracy: 0.55, learning rate: 0.0001\n",
      "Stepping lr_scheduler\n",
      "Epoch 85, train loss: 1.5674, train accuracy: 0.83, test loss: 1.7332, test accuracy: 0.64, learning rate: 1.0000000000000002e-06\n",
      "Saving model!\n",
      "Epoch 86, train loss: 1.5638, train accuracy: 0.83, test loss: 1.7418, test accuracy: 0.64, learning rate: 1.0000000000000002e-06\n",
      "Epoch 87, train loss: 1.5598, train accuracy: 0.83, test loss: 1.7399, test accuracy: 0.64, learning rate: 1.0000000000000002e-06\n",
      "Epoch 88, train loss: 1.5553, train accuracy: 0.84, test loss: 1.7462, test accuracy: 0.63, learning rate: 1.0000000000000002e-06\n",
      "Epoch 89, train loss: 1.5567, train accuracy: 0.83, test loss: 1.7380, test accuracy: 0.63, learning rate: 1.0000000000000002e-06\n",
      "Epoch 90, train loss: 1.5555, train accuracy: 0.84, test loss: 1.7325, test accuracy: 0.66, learning rate: 1.0000000000000002e-06\n",
      "Saving model!\n",
      "Epoch 91, train loss: 1.5531, train accuracy: 0.84, test loss: 1.7547, test accuracy: 0.62, learning rate: 1.0000000000000002e-06\n",
      "Epoch 92, train loss: 1.5526, train accuracy: 0.84, test loss: 1.7358, test accuracy: 0.64, learning rate: 1.0000000000000002e-06\n",
      "Epoch 93, train loss: 1.5534, train accuracy: 0.84, test loss: 1.7410, test accuracy: 0.66, learning rate: 1.0000000000000002e-06\n",
      "Epoch 94, train loss: 1.5533, train accuracy: 0.84, test loss: 1.7380, test accuracy: 0.63, learning rate: 1.0000000000000002e-06\n",
      "Epoch 95, train loss: 1.5514, train accuracy: 0.84, test loss: 1.7516, test accuracy: 0.64, learning rate: 1.0000000000000002e-06\n",
      "Epoch 96, train loss: 1.5489, train accuracy: 0.84, test loss: 1.7372, test accuracy: 0.63, learning rate: 1.0000000000000002e-06\n",
      "Epoch 97, train loss: 1.5535, train accuracy: 0.84, test loss: 1.7501, test accuracy: 0.63, learning rate: 1.0000000000000002e-06\n",
      "Epoch 98, train loss: 1.5523, train accuracy: 0.84, test loss: 1.7829, test accuracy: 0.59, learning rate: 1.0000000000000002e-06\n",
      "Epoch 99, train loss: 1.5499, train accuracy: 0.84, test loss: 1.7320, test accuracy: 0.64, learning rate: 1.0000000000000002e-06\n",
      "Saving model!\n",
      "Epoch 100, train loss: 1.5498, train accuracy: 0.84, test loss: 1.7373, test accuracy: 0.64, learning rate: 1.0000000000000002e-06\n",
      "Epoch 101, train loss: 1.5491, train accuracy: 0.84, test loss: 1.7406, test accuracy: 0.64, learning rate: 1.0000000000000002e-06\n",
      "Epoch 102, train loss: 1.5493, train accuracy: 0.84, test loss: 1.7299, test accuracy: 0.66, learning rate: 1.0000000000000002e-06\n",
      "Saving model!\n",
      "Epoch 103, train loss: 1.5505, train accuracy: 0.84, test loss: 1.7883, test accuracy: 0.60, learning rate: 1.0000000000000002e-06\n",
      "Epoch 104, train loss: 1.5459, train accuracy: 0.84, test loss: 1.7488, test accuracy: 0.62, learning rate: 1.0000000000000002e-06\n",
      "Epoch 105, train loss: 1.5449, train accuracy: 0.84, test loss: 1.7777, test accuracy: 0.61, learning rate: 1.0000000000000002e-06\n",
      "Epoch 106, train loss: 1.5448, train accuracy: 0.84, test loss: 1.7424, test accuracy: 0.63, learning rate: 1.0000000000000002e-06\n",
      "Epoch 107, train loss: 1.5454, train accuracy: 0.84, test loss: 1.7383, test accuracy: 0.65, learning rate: 1.0000000000000002e-06\n",
      "Epoch 108, train loss: 1.5452, train accuracy: 0.84, test loss: 1.7718, test accuracy: 0.61, learning rate: 1.0000000000000002e-06\n",
      "Epoch 109, train loss: 1.5436, train accuracy: 0.85, test loss: 1.7363, test accuracy: 0.64, learning rate: 1.0000000000000002e-06\n",
      "Epoch 110, train loss: 1.5443, train accuracy: 0.84, test loss: 1.7468, test accuracy: 0.64, learning rate: 1.0000000000000002e-06\n",
      "Epoch 111, train loss: 1.5425, train accuracy: 0.84, test loss: 1.7454, test accuracy: 0.65, learning rate: 1.0000000000000002e-06\n",
      "Epoch 112, train loss: 1.5404, train accuracy: 0.84, test loss: 1.7429, test accuracy: 0.64, learning rate: 1.0000000000000002e-06\n",
      "Epoch 113, train loss: 1.5446, train accuracy: 0.84, test loss: 1.7533, test accuracy: 0.63, learning rate: 1.0000000000000002e-06\n",
      "Epoch 114, train loss: 1.5435, train accuracy: 0.84, test loss: 1.7484, test accuracy: 0.64, learning rate: 1.0000000000000002e-06\n",
      "Epoch 115, train loss: 1.5403, train accuracy: 0.84, test loss: 1.7459, test accuracy: 0.63, learning rate: 1.0000000000000002e-06\n",
      "Epoch 116, train loss: 1.5413, train accuracy: 0.85, test loss: 1.8293, test accuracy: 0.54, learning rate: 1.0000000000000002e-06\n",
      "Epoch 117, train loss: 1.5417, train accuracy: 0.84, test loss: 1.7637, test accuracy: 0.60, learning rate: 1.0000000000000002e-06\n",
      "Epoch 118, train loss: 1.5443, train accuracy: 0.84, test loss: 1.7378, test accuracy: 0.63, learning rate: 1.0000000000000002e-06\n",
      "Epoch 119, train loss: 1.5391, train accuracy: 0.85, test loss: 1.7278, test accuracy: 0.66, learning rate: 1.0000000000000002e-06\n",
      "Saving model!\n",
      "Epoch 120, train loss: 1.5338, train accuracy: 0.85, test loss: 1.7390, test accuracy: 0.65, learning rate: 1.0000000000000002e-06\n",
      "Epoch 121, train loss: 1.5354, train accuracy: 0.85, test loss: 1.7338, test accuracy: 0.64, learning rate: 1.0000000000000002e-06\n",
      "Epoch 122, train loss: 1.5382, train accuracy: 0.85, test loss: 1.7370, test accuracy: 0.65, learning rate: 1.0000000000000002e-06\n",
      "Epoch 123, train loss: 1.5377, train accuracy: 0.85, test loss: 1.7262, test accuracy: 0.66, learning rate: 1.0000000000000002e-06\n",
      "Saving model!\n",
      "Epoch 124, train loss: 1.5325, train accuracy: 0.85, test loss: 1.7690, test accuracy: 0.63, learning rate: 1.0000000000000002e-06\n",
      "Epoch 125, train loss: 1.5340, train accuracy: 0.85, test loss: 1.7446, test accuracy: 0.65, learning rate: 1.0000000000000002e-06\n",
      "Epoch 126, train loss: 1.5326, train accuracy: 0.86, test loss: 1.7362, test accuracy: 0.64, learning rate: 1.0000000000000002e-06\n",
      "Epoch 127, train loss: 1.5308, train accuracy: 0.86, test loss: 1.7589, test accuracy: 0.63, learning rate: 1.0000000000000002e-06\n",
      "Epoch 128, train loss: 1.5284, train accuracy: 0.86, test loss: 1.7534, test accuracy: 0.63, learning rate: 1.0000000000000002e-06\n",
      "Epoch 129, train loss: 1.5300, train accuracy: 0.86, test loss: 1.7481, test accuracy: 0.65, learning rate: 1.0000000000000002e-06\n",
      "Epoch 130, train loss: 1.5295, train accuracy: 0.87, test loss: 1.7335, test accuracy: 0.66, learning rate: 1.0000000000000002e-06\n",
      "Epoch 131, train loss: 1.5311, train accuracy: 0.86, test loss: 1.7538, test accuracy: 0.64, learning rate: 1.0000000000000002e-06\n",
      "Epoch 132, train loss: 1.5300, train accuracy: 0.86, test loss: 1.7990, test accuracy: 0.59, learning rate: 1.0000000000000002e-06\n",
      "Epoch 133, train loss: 1.5271, train accuracy: 0.87, test loss: 1.7613, test accuracy: 0.63, learning rate: 1.0000000000000002e-06\n",
      "Epoch 134, train loss: 1.5277, train accuracy: 0.87, test loss: 1.7484, test accuracy: 0.65, learning rate: 1.0000000000000002e-06\n",
      "Epoch 135, train loss: 1.5266, train accuracy: 0.88, test loss: 1.7552, test accuracy: 0.63, learning rate: 1.0000000000000002e-06\n",
      "Epoch 136, train loss: 1.5214, train accuracy: 0.88, test loss: 1.7632, test accuracy: 0.64, learning rate: 1.0000000000000002e-06\n",
      "Epoch 137, train loss: 1.5230, train accuracy: 0.88, test loss: 1.8119, test accuracy: 0.56, learning rate: 1.0000000000000002e-06\n",
      "Epoch 138, train loss: 1.5192, train accuracy: 0.89, test loss: 1.7323, test accuracy: 0.66, learning rate: 1.0000000000000002e-06\n",
      "Epoch 139, train loss: 1.5212, train accuracy: 0.89, test loss: 1.7329, test accuracy: 0.65, learning rate: 1.0000000000000002e-06\n",
      "Epoch 140, train loss: 1.5156, train accuracy: 0.89, test loss: 1.7405, test accuracy: 0.64, learning rate: 1.0000000000000002e-06\n",
      "Epoch 141, train loss: 1.5163, train accuracy: 0.89, test loss: 1.7561, test accuracy: 0.63, learning rate: 1.0000000000000002e-06\n",
      "Epoch 142, train loss: 1.5183, train accuracy: 0.89, test loss: 1.7285, test accuracy: 0.67, learning rate: 1.0000000000000002e-06\n",
      "Epoch 143, train loss: 1.5138, train accuracy: 0.89, test loss: 1.8340, test accuracy: 0.53, learning rate: 1.0000000000000002e-06\n",
      "Stepping lr_scheduler\n",
      "Epoch 144, train loss: 1.5104, train accuracy: 0.90, test loss: 1.7269, test accuracy: 0.66, learning rate: 1.0000000000000002e-07\n",
      "Epoch 145, train loss: 1.5076, train accuracy: 0.90, test loss: 1.7299, test accuracy: 0.65, learning rate: 1.0000000000000002e-07\n",
      "Epoch 146, train loss: 1.5041, train accuracy: 0.91, test loss: 1.7286, test accuracy: 0.65, learning rate: 1.0000000000000002e-07\n",
      "Epoch 147, train loss: 1.5026, train accuracy: 0.91, test loss: 1.7355, test accuracy: 0.65, learning rate: 1.0000000000000002e-07\n",
      "Epoch 148, train loss: 1.5036, train accuracy: 0.91, test loss: 1.7305, test accuracy: 0.66, learning rate: 1.0000000000000002e-07\n",
      "Epoch 149, train loss: 1.5049, train accuracy: 0.91, test loss: 1.7305, test accuracy: 0.65, learning rate: 1.0000000000000002e-07\n",
      "Epoch 150, train loss: 1.5069, train accuracy: 0.91, test loss: 1.7257, test accuracy: 0.66, learning rate: 1.0000000000000002e-07\n",
      "Saving model!\n",
      "Epoch 151, train loss: 1.5040, train accuracy: 0.91, test loss: 1.7305, test accuracy: 0.66, learning rate: 1.0000000000000002e-07\n",
      "Epoch 152, train loss: 1.5079, train accuracy: 0.91, test loss: 1.7308, test accuracy: 0.67, learning rate: 1.0000000000000002e-07\n",
      "Epoch 153, train loss: 1.5068, train accuracy: 0.90, test loss: 1.7482, test accuracy: 0.64, learning rate: 1.0000000000000002e-07\n",
      "Epoch 154, train loss: 1.5063, train accuracy: 0.91, test loss: 1.7310, test accuracy: 0.65, learning rate: 1.0000000000000002e-07\n",
      "Epoch 155, train loss: 1.5023, train accuracy: 0.91, test loss: 1.7306, test accuracy: 0.67, learning rate: 1.0000000000000002e-07\n",
      "Epoch 156, train loss: 1.5047, train accuracy: 0.90, test loss: 1.7310, test accuracy: 0.65, learning rate: 1.0000000000000002e-07\n",
      "Epoch 157, train loss: 1.5040, train accuracy: 0.90, test loss: 1.7408, test accuracy: 0.66, learning rate: 1.0000000000000002e-07\n",
      "Epoch 158, train loss: 1.5032, train accuracy: 0.91, test loss: 1.7296, test accuracy: 0.66, learning rate: 1.0000000000000002e-07\n",
      "Epoch 159, train loss: 1.5025, train accuracy: 0.91, test loss: 1.7287, test accuracy: 0.66, learning rate: 1.0000000000000002e-07\n",
      "Epoch 160, train loss: 1.4994, train accuracy: 0.91, test loss: 1.7332, test accuracy: 0.67, learning rate: 1.0000000000000002e-07\n",
      "Epoch 161, train loss: 1.5027, train accuracy: 0.91, test loss: 1.7277, test accuracy: 0.66, learning rate: 1.0000000000000002e-07\n",
      "Epoch 162, train loss: 1.5053, train accuracy: 0.90, test loss: 1.7328, test accuracy: 0.65, learning rate: 1.0000000000000002e-07\n",
      "Epoch 163, train loss: 1.5002, train accuracy: 0.91, test loss: 1.7516, test accuracy: 0.66, learning rate: 1.0000000000000002e-07\n",
      "Epoch 164, train loss: 1.5040, train accuracy: 0.91, test loss: 1.7343, test accuracy: 0.65, learning rate: 1.0000000000000002e-07\n",
      "Epoch 165, train loss: 1.5015, train accuracy: 0.91, test loss: 1.7331, test accuracy: 0.66, learning rate: 1.0000000000000002e-07\n",
      "Epoch 166, train loss: 1.5036, train accuracy: 0.90, test loss: 1.7309, test accuracy: 0.67, learning rate: 1.0000000000000002e-07\n",
      "Epoch 167, train loss: 1.5013, train accuracy: 0.91, test loss: 1.7308, test accuracy: 0.66, learning rate: 1.0000000000000002e-07\n",
      "Epoch 168, train loss: 1.5013, train accuracy: 0.91, test loss: 1.7282, test accuracy: 0.66, learning rate: 1.0000000000000002e-07\n",
      "Epoch 169, train loss: 1.4985, train accuracy: 0.91, test loss: 1.7352, test accuracy: 0.66, learning rate: 1.0000000000000002e-07\n",
      "Epoch 170, train loss: 1.5014, train accuracy: 0.91, test loss: 1.7271, test accuracy: 0.67, learning rate: 1.0000000000000002e-07\n",
      "Stepping lr_scheduler\n",
      "Epoch 171, train loss: 1.5000, train accuracy: 0.91, test loss: 1.7310, test accuracy: 0.67, learning rate: 1.0000000000000004e-08\n",
      "Epoch 172, train loss: 1.4978, train accuracy: 0.92, test loss: 1.7289, test accuracy: 0.67, learning rate: 1.0000000000000004e-08\n",
      "Epoch 173, train loss: 1.5004, train accuracy: 0.91, test loss: 1.7259, test accuracy: 0.68, learning rate: 1.0000000000000004e-08\n",
      "Epoch 174, train loss: 1.5016, train accuracy: 0.91, test loss: 1.7258, test accuracy: 0.67, learning rate: 1.0000000000000004e-08\n",
      "Epoch 175, train loss: 1.5018, train accuracy: 0.91, test loss: 1.7281, test accuracy: 0.67, learning rate: 1.0000000000000004e-08\n",
      "Epoch 176, train loss: 1.4989, train accuracy: 0.91, test loss: 1.7318, test accuracy: 0.67, learning rate: 1.0000000000000004e-08\n",
      "Epoch 177, train loss: 1.4982, train accuracy: 0.92, test loss: 1.7300, test accuracy: 0.67, learning rate: 1.0000000000000004e-08\n",
      "Epoch 178, train loss: 1.5025, train accuracy: 0.91, test loss: 1.7292, test accuracy: 0.68, learning rate: 1.0000000000000004e-08\n",
      "Epoch 179, train loss: 1.4959, train accuracy: 0.92, test loss: 1.7261, test accuracy: 0.67, learning rate: 1.0000000000000004e-08\n",
      "Epoch 180, train loss: 1.4970, train accuracy: 0.92, test loss: 1.7276, test accuracy: 0.67, learning rate: 1.0000000000000004e-08\n",
      "Stepping lr_scheduler\n",
      "Epoch 181, train loss: 1.4992, train accuracy: 0.91, test loss: 1.7298, test accuracy: 0.66, learning rate: 1.0000000000000005e-09\n",
      "Epoch 182, train loss: 1.5005, train accuracy: 0.91, test loss: 1.7272, test accuracy: 0.66, learning rate: 1.0000000000000005e-09\n",
      "Epoch 183, train loss: 1.5026, train accuracy: 0.91, test loss: 1.7325, test accuracy: 0.67, learning rate: 1.0000000000000005e-09\n",
      "Epoch 184, train loss: 1.5019, train accuracy: 0.91, test loss: 1.7275, test accuracy: 0.67, learning rate: 1.0000000000000005e-09\n",
      "Epoch 185, train loss: 1.4995, train accuracy: 0.91, test loss: 1.7331, test accuracy: 0.67, learning rate: 1.0000000000000005e-09\n",
      "Epoch 186, train loss: 1.5004, train accuracy: 0.91, test loss: 1.8712, test accuracy: 0.51, learning rate: 1.0000000000000005e-09\n",
      "Epoch 187, train loss: 1.5022, train accuracy: 0.91, test loss: 1.7503, test accuracy: 0.66, learning rate: 1.0000000000000005e-09\n",
      "Epoch 188, train loss: 1.4986, train accuracy: 0.92, test loss: 1.9445, test accuracy: 0.42, learning rate: 1.0000000000000005e-09\n",
      "Epoch 189, train loss: 1.4993, train accuracy: 0.91, test loss: 1.7365, test accuracy: 0.67, learning rate: 1.0000000000000005e-09\n",
      "Epoch 190, train loss: 1.5007, train accuracy: 0.91, test loss: 1.7400, test accuracy: 0.66, learning rate: 1.0000000000000005e-09\n",
      "Stepping lr_scheduler\n",
      "Epoch 191, train loss: 1.4995, train accuracy: 0.91, test loss: 1.7281, test accuracy: 0.67, learning rate: 1.0000000000000006e-10\n",
      "Epoch 192, train loss: 1.5007, train accuracy: 0.91, test loss: 1.7270, test accuracy: 0.66, learning rate: 1.0000000000000006e-10\n",
      "Epoch 193, train loss: 1.5004, train accuracy: 0.91, test loss: 1.7260, test accuracy: 0.67, learning rate: 1.0000000000000006e-10\n",
      "Epoch 194, train loss: 1.4994, train accuracy: 0.92, test loss: 1.7309, test accuracy: 0.66, learning rate: 1.0000000000000006e-10\n",
      "Epoch 195, train loss: 1.5000, train accuracy: 0.91, test loss: 1.7496, test accuracy: 0.66, learning rate: 1.0000000000000006e-10\n",
      "Epoch 196, train loss: 1.5008, train accuracy: 0.91, test loss: 1.7323, test accuracy: 0.65, learning rate: 1.0000000000000006e-10\n",
      "Epoch 197, train loss: 1.5033, train accuracy: 0.91, test loss: 1.7289, test accuracy: 0.65, learning rate: 1.0000000000000006e-10\n",
      "Epoch 198, train loss: 1.5009, train accuracy: 0.91, test loss: 1.7459, test accuracy: 0.67, learning rate: 1.0000000000000006e-10\n",
      "Epoch 199, train loss: 1.5001, train accuracy: 0.91, test loss: 1.7259, test accuracy: 0.69, learning rate: 1.0000000000000006e-10\n"
     ]
    }
   ],
   "source": [
    "train_losses, test_losses = train_mel(mel_model, optimizer, scheduler, train_loader_mel, test_loader_mel, num_epochs, patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Training CNN_LSTM Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1690349386972,
     "user": {
      "displayName": "Jeffrey Thomas",
      "userId": "00428686483222286344"
     },
     "user_tz": 420
    },
    "id": "TYeyPSx5PVdW"
   },
   "outputs": [],
   "source": [
    "patience = 60\n",
    "num_epochs = 20\n",
    "num_classes = len(torch.unique(y_train))\n",
    "\n",
    "lrs = [0.01, 0.001, 0.0001, 0.00001]\n",
    "dropout_rates = [0.1, 0.15, 0.2, 0.3, 0.4, 0.5]\n",
    "weight_decays = [0.001, 0.005, 0.01, 0.05]\n",
    "hidden_sizes = [32, 64, 128, 256, 512, 1024]\n",
    "num_lstm_layer_options = [1, 2, 3]\n",
    "num_cnn_layer_options = [3, 4, 5]\n",
    "initial_filter_options = [8, 16, 32, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1690388219398,
     "user": {
      "displayName": "Jeffrey Thomas",
      "userId": "00428686483222286344"
     },
     "user_tz": 420
    },
    "id": "lGl3H-WHPYJo"
   },
   "outputs": [],
   "source": [
    "best_loss = float('inf')\n",
    "best_hyperparameters = None\n",
    "hyperparameters_tested = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1690388273416,
     "user": {
      "displayName": "Jeffrey Thomas",
      "userId": "00428686483222286344"
     },
     "user_tz": 420
    },
    "id": "N8zDcD6uPaYj"
   },
   "outputs": [],
   "source": [
    "def run_training(train_loader_mel, test_loader_mel, num_classes, device, num_epochs, patience, accelerator):\n",
    "    global hyperparameters_tested\n",
    "    global best_loss\n",
    "    global best_hyperparameters\n",
    "\n",
    "    def generate_hypers():\n",
    "        lr = np.random.choice(lrs)\n",
    "        dropout_rate = np.random.choice(dropout_rates)\n",
    "        weight_decay = np.random.choice(weight_decays)\n",
    "        hidden_size = np.random.choice(hidden_sizes)\n",
    "        num_lstm_layers = np.random.choice(num_lstm_layer_options)\n",
    "        num_cnn_layers = np.random.choice(num_cnn_layer_options)\n",
    "        initial_filter = np.random.choice(initial_filter_options)\n",
    "\n",
    "        hyperparameters_dict = {'lr': lr, 'dropout_rate': dropout_rate, 'weight_decay': weight_decay,\n",
    "                          'hidden_size': hidden_size, 'num_lstm_layers': num_lstm_layers,\n",
    "                          'num_cnn_layers': num_cnn_layers, 'initial_filter': initial_filter}\n",
    "\n",
    "    return hyperparameters_dict\n",
    "\n",
    "\n",
    "    hyperparameters_dict = generate_hypers()\n",
    "    while tuple(hyperparameters_dict.values()) in hyperparameters_tested:\n",
    "        hyperparameters_dict = generate_hypers()\n",
    "\n",
    "    hyperparameters_tested.add(tuple(hyperparameters_dict.values()))\n",
    "\n",
    "  print(hyperparameters_dict)\n",
    "\n",
    "  mel_lstm_cnn_model = Mel_LSTM_CNN(num_classes,\n",
    "                                    hyperparameters_dict['hidden_size'],\n",
    "                                    hyperparameters_dict['initial_filter'],\n",
    "                                    hyperparameters_dict['num_lstm_layers'],\n",
    "                                    hyperparameters_dict['num_cnn_layers'],\n",
    "                                    hyperparameters_dict['dropout_rate'])\n",
    "  mel_lstm_cnn_model = mel_lstm_cnn_model.to(device)\n",
    "  optimizer = torch.optim.Adam(mel_lstm_cnn_model.parameters(), lr=hyperparameters_dict['lr'], weight_decay=hyperparameters_dict['weight_decay'])\n",
    "  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "  mel_lstm_cnn_model, optimizer, train_loader_mel, test_loader_mel = accelerator.prepare(\n",
    "    mel_lstm_cnn_model, optimizer, train_loader_mel, test_loader_mel\n",
    "  )\n",
    "\n",
    "\n",
    "  _, _, _, _, best_test_loss = train_mel(mel_lstm_cnn_model,\n",
    "    optimizer, scheduler, train_loader_mel, test_loader_mel, num_epochs, patience)\n",
    "\n",
    "\n",
    "\n",
    "  if best_test_loss < best_loss:\n",
    "    best_loss = best_test_loss\n",
    "    best_hyperparameters = hyperparameters_dict\n",
    "    print(f'Best hyperparameters updated! Best loss: {best_loss}')\n",
    "\n",
    "  return hyperparameters_dict, best_loss, best_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nel7sq46Pcq9",
    "outputId": "cc58928d-d6d8-4dc8-83a1-ff73baf6b9f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.0001, 'dropout_rate': 0.1, 'weight_decay': 0.01, 'hidden_size': 128, 'num_lstm_layers': 3, 'num_cnn_layers': 5, 'initial_filter': 8}\n",
      "Epoch 0, train loss: 2.1971, train accuracy: 0.12, test loss: 2.1951, test accuracy: 0.28, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 1, train loss: 2.1965, train accuracy: 0.21, test loss: 2.1945, test accuracy: 0.28, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 2, train loss: 2.1961, train accuracy: 0.21, test loss: 2.1940, test accuracy: 0.28, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 3, train loss: 2.1956, train accuracy: 0.21, test loss: 2.1935, test accuracy: 0.28, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 4, train loss: 2.1952, train accuracy: 0.21, test loss: 2.1931, test accuracy: 0.28, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 5, train loss: 2.1949, train accuracy: 0.21, test loss: 2.1927, test accuracy: 0.28, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 6, train loss: 2.1945, train accuracy: 0.21, test loss: 2.1922, test accuracy: 0.28, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 7, train loss: 2.1941, train accuracy: 0.21, test loss: 2.1917, test accuracy: 0.28, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 8, train loss: 2.1937, train accuracy: 0.21, test loss: 2.1912, test accuracy: 0.28, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 9, train loss: 2.1932, train accuracy: 0.21, test loss: 2.1906, test accuracy: 0.28, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 10, train loss: 2.1928, train accuracy: 0.21, test loss: 2.1901, test accuracy: 0.28, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 11, train loss: 2.1922, train accuracy: 0.21, test loss: 2.1894, test accuracy: 0.28, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 12, train loss: 2.1917, train accuracy: 0.21, test loss: 2.1887, test accuracy: 0.28, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 13, train loss: 2.1912, train accuracy: 0.21, test loss: 2.1880, test accuracy: 0.28, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 14, train loss: 2.1905, train accuracy: 0.21, test loss: 2.1872, test accuracy: 0.28, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 15, train loss: 2.1899, train accuracy: 0.21, test loss: 2.1863, test accuracy: 0.28, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 16, train loss: 2.1892, train accuracy: 0.21, test loss: 2.1854, test accuracy: 0.28, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 17, train loss: 2.1884, train accuracy: 0.21, test loss: 2.1844, test accuracy: 0.28, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 18, train loss: 2.1876, train accuracy: 0.21, test loss: 2.1832, test accuracy: 0.28, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 19, train loss: 2.1867, train accuracy: 0.21, test loss: 2.1821, test accuracy: 0.28, learning rate: 0.0001\n",
      "Saving model!\n",
      "Best hyperparameters updated! Best loss: 2.182091493486859\n",
      "Run 1/60 completed. Best loss so far: 2.182091493486859, Best hyperparameters so far: {'lr': 0.0001, 'dropout_rate': 0.1, 'weight_decay': 0.01, 'hidden_size': 128, 'num_lstm_layers': 3, 'num_cnn_layers': 5, 'initial_filter': 8}\n",
      "{'lr': 0.01, 'dropout_rate': 0.2, 'weight_decay': 0.01, 'hidden_size': 64, 'num_lstm_layers': 2, 'num_cnn_layers': 3, 'initial_filter': 8}\n",
      "Epoch 0, train loss: 2.1704, train accuracy: 0.19, test loss: 2.1331, test accuracy: 0.28, learning rate: 0.01\n",
      "Saving model!\n",
      "Epoch 1, train loss: 2.1481, train accuracy: 0.21, test loss: 2.1212, test accuracy: 0.28, learning rate: 0.01\n",
      "Saving model!\n",
      "Epoch 2, train loss: 2.1472, train accuracy: 0.21, test loss: 2.1455, test accuracy: 0.28, learning rate: 0.01\n",
      "Epoch 3, train loss: 2.1492, train accuracy: 0.21, test loss: 2.1221, test accuracy: 0.28, learning rate: 0.01\n",
      "Epoch 4, train loss: 2.1483, train accuracy: 0.21, test loss: 2.1376, test accuracy: 0.28, learning rate: 0.01\n",
      "Epoch 5, train loss: 2.1477, train accuracy: 0.21, test loss: 2.1159, test accuracy: 0.28, learning rate: 0.01\n",
      "Saving model!\n",
      "Epoch 6, train loss: 2.1485, train accuracy: 0.21, test loss: 2.1324, test accuracy: 0.28, learning rate: 0.01\n",
      "Epoch 7, train loss: 2.1483, train accuracy: 0.21, test loss: 2.1435, test accuracy: 0.28, learning rate: 0.01\n",
      "Epoch 8, train loss: 2.1512, train accuracy: 0.21, test loss: 2.1305, test accuracy: 0.28, learning rate: 0.01\n",
      "Epoch 9, train loss: 2.1485, train accuracy: 0.21, test loss: 2.1151, test accuracy: 0.28, learning rate: 0.01\n",
      "Saving model!\n",
      "Epoch 10, train loss: 2.1479, train accuracy: 0.21, test loss: 2.1069, test accuracy: 0.28, learning rate: 0.01\n",
      "Saving model!\n",
      "Epoch 11, train loss: 2.1485, train accuracy: 0.21, test loss: 2.1245, test accuracy: 0.28, learning rate: 0.01\n",
      "Epoch 12, train loss: 2.1492, train accuracy: 0.20, test loss: 2.1155, test accuracy: 0.28, learning rate: 0.01\n",
      "Epoch 13, train loss: 2.1481, train accuracy: 0.21, test loss: 2.1258, test accuracy: 0.28, learning rate: 0.01\n",
      "Epoch 14, train loss: 2.1492, train accuracy: 0.21, test loss: 2.1260, test accuracy: 0.28, learning rate: 0.01\n",
      "Epoch 15, train loss: 2.1478, train accuracy: 0.21, test loss: 2.1239, test accuracy: 0.28, learning rate: 0.01\n",
      "Epoch 16, train loss: 2.1486, train accuracy: 0.21, test loss: 2.1251, test accuracy: 0.28, learning rate: 0.01\n",
      "Epoch 17, train loss: 2.1482, train accuracy: 0.21, test loss: 2.1250, test accuracy: 0.28, learning rate: 0.01\n",
      "Epoch 18, train loss: 2.1472, train accuracy: 0.21, test loss: 2.1310, test accuracy: 0.28, learning rate: 0.01\n",
      "Epoch 19, train loss: 2.1492, train accuracy: 0.21, test loss: 2.1181, test accuracy: 0.28, learning rate: 0.01\n",
      "Best hyperparameters updated! Best loss: 2.1068931982607024\n",
      "Run 2/60 completed. Best loss so far: 2.1068931982607024, Best hyperparameters so far: {'lr': 0.01, 'dropout_rate': 0.2, 'weight_decay': 0.01, 'hidden_size': 64, 'num_lstm_layers': 2, 'num_cnn_layers': 3, 'initial_filter': 8}\n",
      "{'lr': 0.0001, 'dropout_rate': 0.3, 'weight_decay': 0.01, 'hidden_size': 256, 'num_lstm_layers': 2, 'num_cnn_layers': 5, 'initial_filter': 32}\n",
      "Epoch 0, train loss: 2.1968, train accuracy: 0.08, test loss: 2.1973, test accuracy: 0.05, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 1, train loss: 2.1962, train accuracy: 0.12, test loss: 2.1967, test accuracy: 0.14, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 2, train loss: 2.1958, train accuracy: 0.18, test loss: 2.1962, test accuracy: 0.14, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 3, train loss: 2.1954, train accuracy: 0.18, test loss: 2.1957, test accuracy: 0.14, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 4, train loss: 2.1950, train accuracy: 0.18, test loss: 2.1952, test accuracy: 0.14, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 5, train loss: 2.1946, train accuracy: 0.18, test loss: 2.1948, test accuracy: 0.14, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 6, train loss: 2.1942, train accuracy: 0.18, test loss: 2.1943, test accuracy: 0.14, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 7, train loss: 2.1938, train accuracy: 0.18, test loss: 2.1938, test accuracy: 0.14, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 8, train loss: 2.1934, train accuracy: 0.18, test loss: 2.1932, test accuracy: 0.14, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 9, train loss: 2.1929, train accuracy: 0.18, test loss: 2.1926, test accuracy: 0.14, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 10, train loss: 2.1924, train accuracy: 0.18, test loss: 2.1920, test accuracy: 0.14, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 11, train loss: 2.1919, train accuracy: 0.18, test loss: 2.1913, test accuracy: 0.14, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 12, train loss: 2.1913, train accuracy: 0.18, test loss: 2.1906, test accuracy: 0.14, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 13, train loss: 2.1906, train accuracy: 0.18, test loss: 2.1898, test accuracy: 0.14, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 14, train loss: 2.1900, train accuracy: 0.18, test loss: 2.1888, test accuracy: 0.14, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 15, train loss: 2.1892, train accuracy: 0.18, test loss: 2.1878, test accuracy: 0.14, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 16, train loss: 2.1883, train accuracy: 0.20, test loss: 2.1867, test accuracy: 0.28, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 17, train loss: 2.1874, train accuracy: 0.21, test loss: 2.1855, test accuracy: 0.28, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 18, train loss: 2.1863, train accuracy: 0.21, test loss: 2.1839, test accuracy: 0.28, learning rate: 0.0001\n",
      "Saving model!\n",
      "Epoch 19, train loss: 2.1851, train accuracy: 0.21, test loss: 2.1823, test accuracy: 0.28, learning rate: 0.0001\n",
      "Saving model!\n",
      "Run 3/60 completed. Best loss so far: 2.1068931982607024, Best hyperparameters so far: {'lr': 0.01, 'dropout_rate': 0.2, 'weight_decay': 0.01, 'hidden_size': 64, 'num_lstm_layers': 2, 'num_cnn_layers': 3, 'initial_filter': 8}\n",
      "{'lr': 0.001, 'dropout_rate': 0.5, 'weight_decay': 0.001, 'hidden_size': 32, 'num_lstm_layers': 2, 'num_cnn_layers': 4, 'initial_filter': 64}\n",
      "Epoch 0, train loss: 2.1684, train accuracy: 0.19, test loss: 2.1426, test accuracy: 0.28, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 1, train loss: 2.1172, train accuracy: 0.24, test loss: 2.0647, test accuracy: 0.31, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 2, train loss: 2.0796, train accuracy: 0.28, test loss: 2.1014, test accuracy: 0.28, learning rate: 0.001\n",
      "Epoch 3, train loss: 2.0696, train accuracy: 0.28, test loss: 2.1008, test accuracy: 0.28, learning rate: 0.001\n",
      "Epoch 4, train loss: 2.0717, train accuracy: 0.28, test loss: 2.0271, test accuracy: 0.38, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 5, train loss: 2.0640, train accuracy: 0.29, test loss: 2.0043, test accuracy: 0.40, learning rate: 0.001\n",
      "Saving model!\n",
      "Epoch 6, train loss: 2.0665, train accuracy: 0.29, test loss: 2.0095, test accuracy: 0.40, learning rate: 0.001\n",
      "Epoch 7, train loss: 2.0605, train accuracy: 0.29, test loss: 2.0298, test accuracy: 0.36, learning rate: 0.001\n",
      "Epoch 8, train loss: 2.0664, train accuracy: 0.28, test loss: 2.0293, test accuracy: 0.39, learning rate: 0.001\n",
      "Epoch 9, train loss: 2.0481, train accuracy: 0.32, test loss: 2.1781, test accuracy: 0.14, learning rate: 0.001\n",
      "Epoch 10, train loss: 2.0628, train accuracy: 0.30, test loss: 2.1080, test accuracy: 0.23, learning rate: 0.001\n",
      "Epoch 11, train loss: 2.0552, train accuracy: 0.30, test loss: 2.0226, test accuracy: 0.36, learning rate: 0.001\n",
      "Epoch 12, train loss: 2.0510, train accuracy: 0.31, test loss: 2.0991, test accuracy: 0.28, learning rate: 0.001\n",
      "Epoch 13, train loss: 2.0444, train accuracy: 0.32, test loss: 2.1062, test accuracy: 0.28, learning rate: 0.001\n",
      "Epoch 14, train loss: 2.0410, train accuracy: 0.32, test loss: 2.0987, test accuracy: 0.28, learning rate: 0.001\n",
      "Epoch 15, train loss: 2.0866, train accuracy: 0.28, test loss: 2.2227, test accuracy: 0.13, learning rate: 0.001\n",
      "Epoch 16, train loss: 2.0431, train accuracy: 0.31, test loss: 2.0814, test accuracy: 0.26, learning rate: 0.001\n",
      "Epoch 17, train loss: 2.0489, train accuracy: 0.32, test loss: 2.0413, test accuracy: 0.33, learning rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "num_trials = 60\n",
    "for i in range(num_trials):\n",
    "  accelerator = Accelerator()\n",
    "  hyperparameters, best_loss, best_hyperparameters = run_training(train_loader_mel, test_loader_mel, num_classes, device, num_epochs, patience, accelerator)\n",
    "  print(f'Run {i+1}/{num_trials} completed. Best loss so far: {best_loss}, Best hyperparameters so far: {best_hyperparameters}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDrQ1EB28gMQ"
   },
   "source": [
    "Best loss so far: 1.938269287971273, Best hyperparameters so far: {'lr': 0.0001, 'dropout_rate': 0.4, 'weight_decay': 0.005, 'hidden_size': 64, 'num_lstm_layers': 1, 'num_cnn_layers': 5, 'initial_filter': 16}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for mel_inputs, labels in test_loader:\n",
    "            mel_inputs = mel_inputs.to(device)  # Move inputs to the device\n",
    "            labels = labels.to(device)  # Move labels to the device\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(mel_inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Update counts\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Usage example\n",
    "val_accuracy = calculate_accuracy(model, val_loader)\n",
    "print(f'Validation Accuracy: {val_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(model, test_loader, num_classes):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for mel_inputs, labels in test_loader:\n",
    "            mel_inputs = mel_inputs.to(device)  # Move inputs to the device\n",
    "            labels = labels.to(device)  # Move labels to the device\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(mel_inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Create the confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions, labels=np.arange(num_classes))\n",
    "    return cm\n",
    "\n",
    "def plot_confusion_matrix(confusion_mtx, labels):\n",
    "    # Normalize the confusion matrix\n",
    "    confusion_mtx = confusion_mtx.astype('float') / confusion_mtx.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    # Create a heatmap using seaborn\n",
    "    sns.heatmap(confusion_mtx, annot=True, fmt='.2f', cmap='Blues', xticklabels=labels, yticklabels=labels, ax=ax)\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "\n",
    "    # Rotate x-axis labels if needed\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(torch.unique(y_dev))\n",
    "confusion_mtx = create_confusion_matrix(model, val_loader, num_classes)\n",
    "class_labels = val_loader.dataset.classes()\n",
    "plot_confusion_matrix(confusion_mtx, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader_mel:\n",
    "        inputs = inputs.to(device)  # Move inputs to the device (e.g., GPU)\n",
    "        labels = labels.to(device)  # Move labels to the device\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = mel_model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Update counts\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader_mel:\n",
    "        inputs = inputs.to(device)  # Move inputs to the device (e.g., GPU)\n",
    "        labels = labels.to(device)  # Move labels to the device\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = mel_model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_predictions, labels=np.arange(num_classes))\n",
    "\n",
    "# Normalize the confusion matrix\n",
    "confusion_mtx = confusion_mtx.astype('float') / confusion_mtx.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "\n",
    "class_labels = val_loader.dataset.classes()\n",
    "# Create a heatmap using seaborn\n",
    "sns.heatmap(confusion_mtx, annot=True, fmt='.2f', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels, ax=ax)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels')\n",
    "ax.set_title('Confusion Matrix')\n",
    "\n",
    "# Rotate x-axis labels if needed\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOUJWzLCGobAI546zASPdEC",
   "mount_file_id": "1BRNgN_JKDJfHFffqSAc_mckufzMmrrpO",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
